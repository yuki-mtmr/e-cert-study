import type { TermExamPoints } from '@/types/glossary';

export const MATH_EXAM_POINTS: TermExamPoints[] = [
  // ===== 確率・統計 (math-prob) =====
  {
    termId: 'conditional-prob',
    points: [
      'P(A|B) = P(A∩B) / P(B) の公式は頻出。分母がP(B)であることに注意。',
      'ベイズの定理の導出に直結するため、条件付き確率の変形を素早くできるようにする。',
      '条件付き確率と同時確率の違いを問う問題が出やすい。',
    ],
    formula: 'P(A|B) = P(A ∩ B) / P(B)',
  },
  {
    termId: 'bayes-theorem',
    points: [
      '公式 P(θ|D) = P(D|θ)P(θ) / P(D) の各項（事後確率・尤度・事前確率・周辺尤度）の意味を正確に答えられるようにする。',
      '分母の周辺尤度 P(D) は正規化定数であり、パラメータθに依存しない点がMAP推定・MLEとの関係で問われる。',
      'ナイーブベイズ分類器の理論的基盤として出題されることがある。',
      '具体的な数値を代入して事後確率を計算する問題が頻出。',
    ],
    formula: 'P(θ|D) = P(D|θ)P(θ) / P(D)',
  },
  {
    termId: 'prior-prob',
    points: [
      'MAP推定では事前分布が正則化項として機能する。ガウス分布の事前分布はL2正則化に対応する。',
      '無情報事前分布（一様分布）を用いた場合、MAP推定はMLEと一致する点が問われやすい。',
      '共役事前分布の概念（ベータ分布とベルヌーイ分布の関係など）も出題範囲。',
    ],
  },
  {
    termId: 'posterior-prob',
    points: [
      '事後確率 ∝ 尤度 × 事前確率 という比例関係を覚える。正規化定数を省略できる場面を理解する。',
      'MAP推定は事後確率を最大化するパラメータを求める手法であり、点推定である点に注意。',
      'ベイズ推定では事後分布全体を利用するが、MAP推定は最頻値のみを使う違いを理解する。',
    ],
  },
  {
    termId: 'likelihood',
    points: [
      '尤度はパラメータの関数であり、確率分布ではない点に注意。尤度を積分しても1にならない。',
      '対数尤度に変換して和の形にする計算問題が頻出。積の微分を避けるための工夫。',
      'i.i.d.（独立同分布）仮定の下では尤度は各データの確率の積になる。',
    ],
    formula: 'L(θ) = ∏ P(xᵢ|θ)',
  },
  {
    termId: 'mle',
    points: [
      '対数尤度を微分して0とおく計算手順が頻出。ガウス分布のパラメータ推定が典型例。',
      'MLEはデータ数が多いと過学習しやすい。正則化がない点がMAP推定との違い。',
      'ガウス分布のMLEでは平均は不偏推定量だが、分散はn-1ではなくnで割る（偏り推定量）点が問われる。',
      '交差エントロピー損失の最小化はMLEと等価であることを理解する。',
    ],
  },
  {
    termId: 'map-estimation',
    points: [
      'MAP推定 = MLE + 事前分布（正則化項）という関係を理解する。',
      'ガウス事前分布 → L2正則化、ラプラス事前分布 → L1正則化に対応する。',
      '事前分布が一様分布のときMAP推定はMLEに一致する。この関係は頻出。',
    ],
    formula: 'θ_MAP = argmax P(θ|D) = argmax P(D|θ)P(θ)',
  },
  {
    termId: 'expectation',
    points: [
      '離散の場合 E[X] = Σ xP(x)、連続の場合 E[X] = ∫ xf(x)dx の使い分けを問われる。',
      '期待値の線形性 E[aX+bY] = aE[X]+bE[Y] は独立性を仮定しなくても成立する点に注意。',
      '分散の公式 V[X] = E[X²] - (E[X])² の導出で期待値の性質を使う問題が出る。',
    ],
    formula: 'E[X] = Σ xᵢP(xᵢ)  /  E[X] = ∫ xf(x)dx',
  },
  {
    termId: 'variance',
    points: [
      'V[X] = E[(X-μ)²] = E[X²] - (E[X])² の2つの表現を使い分ける計算問題が頻出。',
      'V[aX+b] = a²V[X] であり、定数の加算は分散に影響しない点が問われる。',
      '独立な確率変数の和の分散 V[X+Y] = V[X]+V[Y] は独立性が必要。共分散の項を忘れやすい。',
    ],
    formula: 'V[X] = E[X²] - (E[X])²',
  },
  {
    termId: 'gaussian',
    points: [
      '確率密度関数の形と、平均μ・分散σ²がそれぞれ分布の中心・広がりを決める点を理解する。',
      'ガウス分布の再生性（ガウス分布の和もガウス分布）は出題されやすい。',
      'ガウス分布を仮定した場合のMLEが最小二乗法と等価になることを導出できるようにする。',
      '多次元ガウス分布の共分散行列の意味と、対角成分・非対角成分の役割を理解する。',
    ],
    formula: 'f(x) = (1/√(2πσ²)) exp(-(x-μ)² / 2σ²)',
  },
  {
    termId: 'bernoulli',
    points: [
      'パラメータpの1つで特徴づけられ、E[X]=p, V[X]=p(1-p) である。',
      '二値分類のロジスティック回帰の出力はベルヌーイ分布に従うと仮定している。',
      'バイナリ交差エントロピー損失はベルヌーイ分布の負の対数尤度と等価。',
    ],
    formula: 'P(X=k) = p^k(1-p)^(1-k),  k ∈ {0,1}',
  },
  {
    termId: 'multinomial',
    points: [
      '多クラス分類でソフトマックス出力をモデル化する分布。カテゴリカル分布との違いを理解する。',
      'カテゴリカル交差エントロピー損失はカテゴリカル分布の負の対数尤度に対応する。',
      'ディリクレ分布が多項分布の共役事前分布である点が出題されることがある。',
    ],
  },

  // ===== 情報理論 (math-info) =====
  {
    termId: 'entropy',
    points: [
      'H(P) = -Σ P(x)log P(x) の公式を正確に書けるようにする。logの底は2（ビット）またはe（ナット）。',
      '一様分布のとき最大エントロピーとなる。確率が偏るほどエントロピーは小さくなる。',
      '決定木の分岐基準（情報利得）の計算でエントロピーを使う問題が出る。',
      '交差エントロピーとの関係: H(P,Q) = H(P) + D_KL(P||Q) を理解する。',
    ],
    formula: 'H(P) = -Σ P(x) log P(x)',
  },
  {
    termId: 'cross-entropy',
    points: [
      '分類タスクの損失関数として最も重要。二値分類ではバイナリ交差エントロピー、多クラスではカテゴリカル交差エントロピーを使う。',
      'H(P,Q) = -Σ P(x)log Q(x) で、PがラベルQがモデル予測。PとQの順序を間違えやすい。',
      '真の分布Pが固定なら、交差エントロピーの最小化はKLダイバージェンスの最小化と等価。',
    ],
    formula: 'H(P, Q) = -Σ P(x) log Q(x)',
  },
  {
    termId: 'kl-divergence',
    points: [
      'D_KL(P||Q) ≥ 0（ギブスの不等式）であり、P=Qのとき0になる。非対称性に注意。',
      'D_KL(P||Q) ≠ D_KL(Q||P) であるため距離（メトリック）ではない。この非対称性は頻出。',
      'VAEの損失関数でKLダイバージェンスが正則化項として使われる（潜在変数を標準正規分布に近づける）。',
      'フォワードKLとリバースKLの違い（モード平均化 vs モード選択）が問われることがある。',
    ],
    formula: 'D_KL(P||Q) = Σ P(x) log(P(x) / Q(x))',
  },
  {
    termId: 'js-divergence',
    points: [
      'KLダイバージェンスを対称化した指標。JSD(P||Q) = (D_KL(P||M) + D_KL(Q||M)) / 2 でM=(P+Q)/2。',
      'GANの元論文の損失関数はJSダイバージェンスの最小化と等価。学習不安定性の原因として議論される。',
      '値域が有界（0以上log2以下）であり、KLダイバージェンスと異なり発散しない。',
    ],
    formula: 'JSD(P||Q) = (D_KL(P||M) + D_KL(Q||M)) / 2,  M = (P+Q)/2',
  },
  {
    termId: 'mutual-info',
    points: [
      'I(X;Y) = H(X) - H(X|Y) = H(Y) - H(Y|X) であり、XとYが独立なら0になる。',
      '特徴選択でターゲット変数との相互情報量が大きい特徴を選ぶ手法が出題される。',
      'KLダイバージェンスとの関係: I(X;Y) = D_KL(P(X,Y) || P(X)P(Y)) を理解する。',
    ],
    formula: 'I(X;Y) = H(X) - H(X|Y)',
  },
  {
    termId: 'self-info',
    points: [
      'I(x) = -log P(x) で定義。確率が低いほど情報量が大きい。エントロピーは自己情報量の期待値。',
      '確率1の事象の自己情報量は0、確率0に近づくと無限大になる直感を理解する。',
    ],
    formula: 'I(x) = -log P(x)',
  },
  {
    termId: 'info-gain',
    points: [
      'IG(S,A) = H(S) - Σ(|Sv|/|S|)H(Sv) で、特徴Aによる分割前後のエントロピー減少量を計算する。',
      '決定木（ID3, C4.5）の分岐基準として出題される。具体的な数値計算問題が頻出。',
      '多値の特徴量に有利になるバイアスがあり、C4.5では情報利得比で補正する点を理解する。',
    ],
    formula: 'IG(S, A) = H(S) - Σ (|Sᵥ|/|S|) H(Sᵥ)',
  },
  {
    termId: 'perplexity',
    points: [
      'PPL = 2^H(P,Q) または exp(H(P,Q)) で定義。交差エントロピーの指数変換。',
      '言語モデルの評価指標として頻出。パープレキシティが低いほど良いモデル。',
      '「次の単語の候補が平均何個あるか」という直感的解釈を理解する。',
    ],
    formula: 'PPL = 2^(H(P,Q))  /  PPL = exp(H(P,Q))',
  },

  // ===== 線形代数 (math-linalg) =====
  {
    termId: 'eigenvalue',
    points: [
      'Av = λv を満たすλが固有値。det(A-λI) = 0（固有方程式）から求める手順を練習する。',
      'PCAでは共分散行列の固有値が各主成分の分散（寄与率）に対応する。',
      '固有値の和 = トレース、固有値の積 = 行列式 という関係は覚えておく。',
    ],
    formula: 'Av = λv  →  det(A - λI) = 0',
  },
  {
    termId: 'eigenvector',
    points: [
      '固有値λに対応する固有ベクトルは (A-λI)v = 0 の非自明な解として求める。',
      'PCAでは共分散行列の固有ベクトルが主成分の方向を表す。固有値が大きい順に第1主成分、第2主成分。',
      '対称行列の固有ベクトルは互いに直交する性質を利用した問題が出る。',
    ],
    formula: '(A - λI)v = 0',
  },
  {
    termId: 'svd',
    points: [
      'A = UΣV^T の分解で、Uは左特異ベクトル、Σは特異値（対角行列）、Vは右特異ベクトル。',
      '任意の行列（正方でなくてもよい）に適用できる点が固有値分解との違い。',
      '特異値を大きい順にk個だけ残す低ランク近似がデータ圧縮・次元削減に使われる。',
      'A^TAの固有値の平方根が特異値である関係を理解する。',
    ],
    formula: 'A = UΣVᵀ',
  },
  {
    termId: 'pca',
    points: [
      '共分散行列の固有値分解で主成分を求める手順を実際に計算できるようにする。',
      '寄与率 = 各固有値 / 固有値の総和。累積寄与率が閾値を超えるまでの主成分数を選ぶ。',
      'データの標準化（平均0、分散1）を事前に行う必要がある。スケールが異なる特徴量に影響される。',
      '次元削減後のデータは元の特徴量の線形結合であり、解釈性が低下する欠点を理解する。',
    ],
  },
  {
    termId: 'matrix-multiply',
    points: [
      '(m×n)行列と(n×p)行列の積は(m×p)行列。内側の次元が一致しないと計算不可。',
      'AB ≠ BA（非可換）である点は頻出の引っかけ問題。',
      'ニューラルネットの順伝播 y = Wx + b は行列積。バッチ処理での次元の対応関係を理解する。',
    ],
  },
  {
    termId: 'transpose',
    points: [
      '(AB)^T = B^T A^T の転置の順序反転は頻出。3つ以上の積でも同様に逆順になる。',
      '逆伝播の計算で重み行列の転置が現れる。全結合層の勾配計算で使う。',
      '対称行列は A = A^T を満たす。共分散行列は対称行列である。',
    ],
    formula: '(AB)ᵀ = BᵀAᵀ',
  },
  {
    termId: 'inverse-matrix',
    points: [
      'AA⁻¹ = A⁻¹A = I を満たす行列。正則（det(A)≠0）でないと逆行列は存在しない。',
      '正規方程式 θ = (X^TX)⁻¹X^Ty で逆行列が使われる。X^TXが正則でない場合の対処法（正則化）も理解する。',
      '2×2行列の逆行列の公式は暗記しておく。ad-bcで割り、成分を入れ替える。',
    ],
    formula: 'A⁻¹ = (1/det(A)) adj(A)  /  2×2: [a b; c d]⁻¹ = (1/(ad-bc))[d -b; -c a]',
  },
  {
    termId: 'determinant',
    points: [
      '2×2行列 det = ad-bc、3×3は余因子展開で計算。計算問題として頻出。',
      'det(A) = 0 なら行列は特異（逆行列が存在しない）。正則性の判定に使う。',
      'det(AB) = det(A)det(B)、det(A^T) = det(A) の性質を利用する問題が出る。',
      '固有値の積が行列式に等しいことを利用した計算問題もある。',
    ],
    formula: 'det([a b; c d]) = ad - bc',
  },
  {
    termId: 'norm',
    points: [
      'L1ノルム（マンハッタン距離）はスパース性を促進し、L2ノルム（ユークリッド距離）は滑らかな解を促進する。',
      'L1正則化（Lasso）とL2正則化（Ridge）の違いとそれぞれのノルムの関係を理解する。',
      '||x||₂ = √(Σxᵢ²) の計算と、勾配降下法でのパラメータ更新時の正則化項への影響を把握する。',
    ],
    formula: '||x||₁ = Σ|xᵢ|,  ||x||₂ = √(Σxᵢ²)',
  },
  {
    termId: 'hadamard',
    points: [
      '要素ごとの積（⊙）であり、通常の行列積とは異なる。同じサイズの行列同士でのみ定義される。',
      'LSTMの忘却ゲート・入力ゲート・出力ゲートでアダマール積が使われる。ゲートのシグモイド出力との要素積。',
      'NumPyでは * 演算子がアダマール積、@ が行列積に対応する。実装問題で混同しやすい。',
    ],
    formula: '(A ⊙ B)ᵢⱼ = Aᵢⱼ × Bᵢⱼ',
  },
];
