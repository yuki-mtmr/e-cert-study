import type { GlossaryTerm } from '@/types/glossary';

export const DL_BASIC_TERMS: GlossaryTerm[] = [
  // 順伝播型NN
  { id: 'perceptron', jaName: 'パーセプトロン', enName: 'Perceptron', description: '単一のニューロンで構成される最も基本的なニューラルネットワーク。線形分離可能な問題のみ解ける。', sectionId: 'dl-basic', subsectionId: 'dl-ffnn' },
  { id: 'mlp', jaName: '多層パーセプトロン', enName: 'Multi-Layer Perceptron', description: '入力層・隠れ層・出力層からなる順伝播型ニューラルネットワーク。非線形問題も学習可能。', sectionId: 'dl-basic', subsectionId: 'dl-ffnn' },
  { id: 'forward-prop', jaName: '順伝播', enName: 'Forward Propagation', description: '入力から出力へ向かって各層の計算を順に行う処理。推論時の基本操作。', sectionId: 'dl-basic', subsectionId: 'dl-ffnn' },
  { id: 'backprop', jaName: '誤差逆伝播法', enName: 'Backpropagation', description: '出力層から入力層へ向かって勾配を効率的に計算するアルゴリズム。連鎖律に基づく。', sectionId: 'dl-basic', subsectionId: 'dl-ffnn' },
  { id: 'gradient', jaName: '勾配', enName: 'Gradient', description: '損失関数の各パラメータに対する偏微分のベクトル。最適化の方向を決定する。', sectionId: 'dl-basic', subsectionId: 'dl-ffnn' },
  { id: 'chain-rule', jaName: '連鎖律', enName: 'Chain Rule', description: '合成関数の微分法則。誤差逆伝播法の数学的基盤となる。', sectionId: 'dl-basic', subsectionId: 'dl-ffnn' },
  { id: 'computational-graph', jaName: '計算グラフ', enName: 'Computational Graph', description: '計算の依存関係を有向グラフで表現したもの。自動微分の基盤となるデータ構造。', sectionId: 'dl-basic', subsectionId: 'dl-ffnn' },
  { id: 'hidden-layer', jaName: '隠れ層', enName: 'Hidden Layer', description: '入力層と出力層の間にある層。層を深くすることで複雑な特徴を学習できる。', sectionId: 'dl-basic', subsectionId: 'dl-ffnn' },
  { id: 'neuron', jaName: 'ニューロン', enName: 'Neuron', description: 'ニューラルネットワークの基本処理単位。入力の重み付き和に活性化関数を適用する。', sectionId: 'dl-basic', subsectionId: 'dl-ffnn' },
  { id: 'weight', jaName: '重み', enName: 'Weight', description: '入力信号の重要度を決定する学習可能なパラメータ。勾配降下法で更新される。', sectionId: 'dl-basic', subsectionId: 'dl-ffnn' },
  { id: 'bias', jaName: 'バイアス', enName: 'Bias', description: '活性化関数への入力をシフトする学習可能なパラメータ。表現力の向上に寄与する。', sectionId: 'dl-basic', subsectionId: 'dl-ffnn' },
  { id: 'universal-approx', jaName: '万能近似定理', enName: 'Universal Approximation Theorem', description: '十分な幅の隠れ層を持つNNは任意の連続関数を近似できるという定理。', sectionId: 'dl-basic', subsectionId: 'dl-ffnn' },
  // 活性化関数
  { id: 'sigmoid', jaName: 'シグモイド関数', enName: 'Sigmoid Function', description: '出力を0〜1に圧縮するS字型の活性化関数。勾配消失問題が発生しやすい。', sectionId: 'dl-basic', subsectionId: 'dl-activation' },
  { id: 'tanh', jaName: 'tanh関数', enName: 'Hyperbolic Tangent', description: '出力を-1〜1に圧縮する活性化関数。シグモイドより原点対称で学習が安定しやすい。', sectionId: 'dl-basic', subsectionId: 'dl-activation' },
  { id: 'relu', jaName: 'ReLU', enName: 'Rectified Linear Unit', description: '負の入力を0、正の入力をそのまま出力する活性化関数。勾配消失が起きにくく広く使用される。', sectionId: 'dl-basic', subsectionId: 'dl-activation' },
  { id: 'leaky-relu', jaName: 'Leaky ReLU', enName: 'Leaky ReLU', description: '負の入力に小さな傾きを持たせたReLU。Dying ReLU問題を緩和する。', sectionId: 'dl-basic', subsectionId: 'dl-activation' },
  { id: 'prelu', jaName: 'PReLU', enName: 'Parametric ReLU', description: '負の領域の傾きを学習可能なパラメータとしたReLUの拡張。', sectionId: 'dl-basic', subsectionId: 'dl-activation' },
  { id: 'elu', jaName: 'ELU', enName: 'Exponential Linear Unit', description: '負の入力に指数関数を用いる活性化関数。出力の平均が0に近づきやすい。', sectionId: 'dl-basic', subsectionId: 'dl-activation' },
  { id: 'gelu', jaName: 'GELU', enName: 'Gaussian Error Linear Unit', description: 'ガウス分布の累積分布関数に基づく活性化関数。TransformerやBERTで標準的に使用。', sectionId: 'dl-basic', subsectionId: 'dl-activation' },
  { id: 'swish', jaName: 'Swish', enName: 'Swish', description: 'x * sigmoid(x)で定義される活性化関数。ReLUより滑らかで深いネットワークで性能が良い。', sectionId: 'dl-basic', subsectionId: 'dl-activation' },
  { id: 'softmax', jaName: 'ソフトマックス関数', enName: 'Softmax Function', description: '出力を確率分布に変換する関数。多クラス分類の出力層で使用される。', sectionId: 'dl-basic', subsectionId: 'dl-activation' },
  { id: 'softplus', jaName: 'Softplus', enName: 'Softplus', description: 'ReLUの滑らかな近似。log(1 + exp(x))で定義される。微分がシグモイド関数になる。', sectionId: 'dl-basic', subsectionId: 'dl-activation' },
  { id: 'mish', jaName: 'Mish', enName: 'Mish', description: 'x * tanh(softplus(x))で定義される自己正則化活性化関数。滑らかで非単調な特性を持つ。', sectionId: 'dl-basic', subsectionId: 'dl-activation' },
  { id: 'dying-relu', jaName: 'Dying ReLU問題', enName: 'Dying ReLU Problem', description: 'ReLUの負の領域で勾配が0になり、ニューロンが永久に不活性化する問題。', sectionId: 'dl-basic', subsectionId: 'dl-activation' },
  // 損失関数
  { id: 'mse-loss', jaName: '平均二乗誤差損失', enName: 'Mean Squared Error Loss', description: '予測値と正解値の差の二乗の平均。回帰タスクの標準的な損失関数。', sectionId: 'dl-basic', subsectionId: 'dl-loss' },
  { id: 'cross-entropy-loss', jaName: '交差エントロピー損失', enName: 'Cross-Entropy Loss', description: '予測確率分布と正解分布の交差エントロピー。分類タスクの標準的な損失関数。', sectionId: 'dl-basic', subsectionId: 'dl-loss' },
  { id: 'binary-cross-entropy', jaName: 'バイナリ交差エントロピー', enName: 'Binary Cross-Entropy', description: '二値分類に特化した交差エントロピー損失関数。シグモイド出力と組み合わせて使用する。', sectionId: 'dl-basic', subsectionId: 'dl-loss' },
  { id: 'hinge-loss', jaName: 'ヒンジ損失', enName: 'Hinge Loss', description: 'SVMで使用される損失関数。マージンを最大化するように学習する。', sectionId: 'dl-basic', subsectionId: 'dl-loss' },
  { id: 'huber-loss', jaName: 'Huber損失', enName: 'Huber Loss', description: 'MSEとMAEのハイブリッド損失関数。外れ値に対してロバストかつ微分可能。', sectionId: 'dl-basic', subsectionId: 'dl-loss' },
  { id: 'focal-loss', jaName: 'Focal Loss', enName: 'Focal Loss', description: '簡単なサンプルの損失を小さくし困難なサンプルに注力する損失関数。クラス不均衡対策に有効。', sectionId: 'dl-basic', subsectionId: 'dl-loss' },
  { id: 'triplet-loss', jaName: 'トリプレット損失', enName: 'Triplet Loss', description: 'アンカー・正例・負例の3つ組で距離学習を行う損失関数。顔認識等で使用される。', sectionId: 'dl-basic', subsectionId: 'dl-loss' },
  { id: 'contrastive-loss', jaName: 'コントラスティブ損失', enName: 'Contrastive Loss', description: '類似ペアの距離を近づけ、非類似ペアの距離を遠ざける損失関数。', sectionId: 'dl-basic', subsectionId: 'dl-loss' },
  { id: 'kl-loss', jaName: 'KLダイバージェンス損失', enName: 'KL Divergence Loss', description: '予測分布と目標分布のKLダイバージェンス。VAEの損失関数の一部として使用される。', sectionId: 'dl-basic', subsectionId: 'dl-loss' },
  { id: 'label-smoothing', jaName: 'ラベルスムージング', enName: 'Label Smoothing', description: 'ハードラベルを少しソフトにする正則化手法。モデルの過信を防ぎ汎化性能を向上させる。', sectionId: 'dl-basic', subsectionId: 'dl-loss' },
  // 最適化
  { id: 'sgd', jaName: '確率的勾配降下法', enName: 'Stochastic Gradient Descent', description: 'ミニバッチごとに勾配を計算してパラメータを更新する最適化手法。深層学習の基本。', sectionId: 'dl-basic', subsectionId: 'dl-optim' },
  { id: 'momentum', jaName: 'モメンタム', enName: 'Momentum', description: '過去の勾配の移動平均を利用して更新を加速する手法。局所最適解の回避に有効。', sectionId: 'dl-basic', subsectionId: 'dl-optim' },
  { id: 'nesterov', jaName: 'ネステロフの加速勾配法', enName: 'Nesterov Accelerated Gradient', description: '先読み位置で勾配を計算するモメンタムの改良版。収束が速い。', sectionId: 'dl-basic', subsectionId: 'dl-optim' },
  { id: 'adagrad', jaName: 'AdaGrad', enName: 'AdaGrad', description: 'パラメータごとに学習率を適応的に調整する手法。更新が多いパラメータの学習率が減衰する。', sectionId: 'dl-basic', subsectionId: 'dl-optim' },
  { id: 'rmsprop', jaName: 'RMSProp', enName: 'RMSProp', description: 'AdaGradの学習率減衰を指数移動平均で緩和した手法。RNNの学習で効果的。', sectionId: 'dl-basic', subsectionId: 'dl-optim' },
  { id: 'adam', jaName: 'Adam', enName: 'Adam', description: 'モメンタムとRMSPropを組み合わせた適応的学習率最適化手法。最も広く使用される。', sectionId: 'dl-basic', subsectionId: 'dl-optim' },
  { id: 'adamw', jaName: 'AdamW', enName: 'AdamW', description: 'Adamに正しい重み減衰を組み込んだ最適化手法。Transformerの学習で標準的に使用。', sectionId: 'dl-basic', subsectionId: 'dl-optim' },
  { id: 'learning-rate', jaName: '学習率', enName: 'Learning Rate', description: 'パラメータ更新の大きさを制御するハイパーパラメータ。学習の収束速度と安定性に影響。', sectionId: 'dl-basic', subsectionId: 'dl-optim' },
  { id: 'lr-schedule', jaName: '学習率スケジューリング', enName: 'Learning Rate Scheduling', description: '学習の進行に応じて学習率を変化させる手法。コサインアニーリング等が代表的。', sectionId: 'dl-basic', subsectionId: 'dl-optim' },
  { id: 'warmup', jaName: 'ウォームアップ', enName: 'Warmup', description: '学習初期に学習率を徐々に上げる手法。Transformer等の大規模モデルで安定した学習に寄与。', sectionId: 'dl-basic', subsectionId: 'dl-optim' },
  { id: 'gradient-clipping', jaName: '勾配クリッピング', enName: 'Gradient Clipping', description: '勾配のノルムを閾値で制限する手法。勾配爆発を防止しRNNの学習安定化に有効。', sectionId: 'dl-basic', subsectionId: 'dl-optim' },
  { id: 'vanishing-gradient', jaName: '勾配消失問題', enName: 'Vanishing Gradient Problem', description: '深い層で勾配が極めて小さくなり学習が進まなくなる問題。ReLUや残差接続で緩和される。', sectionId: 'dl-basic', subsectionId: 'dl-optim' },
  { id: 'exploding-gradient', jaName: '勾配爆発問題', enName: 'Exploding Gradient Problem', description: '勾配が指数的に増大し学習が不安定になる問題。勾配クリッピングで対処する。', sectionId: 'dl-basic', subsectionId: 'dl-optim' },
  { id: 'batch-size', jaName: 'バッチサイズ', enName: 'Batch Size', description: '1回のパラメータ更新に使用するサンプル数。学習の速度と汎化性能に影響する。', sectionId: 'dl-basic', subsectionId: 'dl-optim' },
  // 重み初期化
  { id: 'xavier-init', jaName: 'Xavier初期化', enName: 'Xavier Initialization', description: '入出力ユニット数に基づく初期化手法。シグモイドやtanhと組み合わせて使用する。', sectionId: 'dl-basic', subsectionId: 'dl-init' },
  { id: 'he-init', jaName: 'He初期化', enName: 'He Initialization', description: 'ReLU活性化関数に適した初期化手法。分散を2/nで設定し勾配の消失・爆発を防ぐ。', sectionId: 'dl-basic', subsectionId: 'dl-init' },
  { id: 'lecun-init', jaName: 'LeCun初期化', enName: 'LeCun Initialization', description: 'SELU活性化関数と組み合わせた自己正規化ネットワーク向けの初期化手法。', sectionId: 'dl-basic', subsectionId: 'dl-init' },
  { id: 'zero-init', jaName: 'ゼロ初期化', enName: 'Zero Initialization', description: '全重みを0に初期化する方法。対称性の問題で隠れ層には適さないが、バイアスには使用される。', sectionId: 'dl-basic', subsectionId: 'dl-init' },
  { id: 'random-init', jaName: 'ランダム初期化', enName: 'Random Initialization', description: '重みをランダムな小さい値で初期化する方法。対称性を破るために必要。', sectionId: 'dl-basic', subsectionId: 'dl-init' },
  { id: 'orthogonal-init', jaName: '直交初期化', enName: 'Orthogonal Initialization', description: '直交行列で重みを初期化する手法。RNNの学習安定化に特に有効。', sectionId: 'dl-basic', subsectionId: 'dl-init' },
  // 正則化
  { id: 'l1-reg', jaName: 'L1正則化', enName: 'L1 Regularization', description: '重みの絶対値の和をペナルティとして加える正則化。スパースな解を得やすい。', sectionId: 'dl-basic', subsectionId: 'dl-reg' },
  { id: 'l2-reg', jaName: 'L2正則化', enName: 'L2 Regularization', description: '重みの二乗和をペナルティとして加える正則化。重みを小さく保ち過学習を抑制する。', sectionId: 'dl-basic', subsectionId: 'dl-reg' },
  { id: 'dropout', jaName: 'ドロップアウト', enName: 'Dropout', description: '学習時にランダムにニューロンを無効化する正則化手法。アンサンブル効果がある。', sectionId: 'dl-basic', subsectionId: 'dl-reg' },
  { id: 'batch-norm', jaName: 'バッチ正規化', enName: 'Batch Normalization', description: 'ミニバッチ内で各層の出力を正規化する手法。学習を安定化し収束を高速化する。', sectionId: 'dl-basic', subsectionId: 'dl-reg' },
  { id: 'layer-norm', jaName: 'レイヤー正規化', enName: 'Layer Normalization', description: '各サンプルの層全体で正規化する手法。バッチサイズに依存せずTransformerで標準的。', sectionId: 'dl-basic', subsectionId: 'dl-reg' },
  { id: 'group-norm', jaName: 'グループ正規化', enName: 'Group Normalization', description: 'チャネルをグループに分けて正規化する手法。小バッチでBatch Normより安定。', sectionId: 'dl-basic', subsectionId: 'dl-reg' },
  { id: 'instance-norm', jaName: 'インスタンス正規化', enName: 'Instance Normalization', description: '各サンプルの各チャネルごとに正規化する手法。スタイル変換で使用される。', sectionId: 'dl-basic', subsectionId: 'dl-reg' },
  { id: 'weight-decay', jaName: '重み減衰', enName: 'Weight Decay', description: '各更新ステップで重みを一定割合縮小する正則化手法。L2正則化と関連するが区別が重要。', sectionId: 'dl-basic', subsectionId: 'dl-reg' },
  { id: 'spectral-norm', jaName: 'スペクトル正規化', enName: 'Spectral Normalization', description: '重み行列のスペクトルノルムを1に制約する正則化。GANの学習安定化に有効。', sectionId: 'dl-basic', subsectionId: 'dl-reg' },
  { id: 'mixup', jaName: 'Mixup', enName: 'Mixup', description: '2つの訓練サンプルを線形補間して新しいデータを生成するデータ拡張手法。', sectionId: 'dl-basic', subsectionId: 'dl-reg' },
  { id: 'cutout', jaName: 'Cutout', enName: 'Cutout', description: '画像のランダムな矩形領域をマスクするデータ拡張手法。遮蔽に対する頑健性を向上。', sectionId: 'dl-basic', subsectionId: 'dl-reg' },
  { id: 'cutmix', jaName: 'CutMix', enName: 'CutMix', description: '画像の一部を別の画像で置き換えるデータ拡張手法。Cutoutとmixupの利点を組み合わせる。', sectionId: 'dl-basic', subsectionId: 'dl-reg' },
  // CNN
  { id: 'convolution', jaName: '畳み込み', enName: 'Convolution', description: 'フィルタを入力にスライドさせて局所的な特徴を抽出する演算。CNNの基本操作。', sectionId: 'dl-basic', subsectionId: 'dl-cnn' },
  { id: 'filter-kernel', jaName: 'フィルタ（カーネル）', enName: 'Filter (Kernel)', description: '畳み込み演算で使用する小さな重み行列。エッジやテクスチャなどの特徴を検出する。', sectionId: 'dl-basic', subsectionId: 'dl-cnn' },
  { id: 'stride', jaName: 'ストライド', enName: 'Stride', description: 'フィルタをスライドさせる間隔。大きいストライドで出力サイズが小さくなる。', sectionId: 'dl-basic', subsectionId: 'dl-cnn' },
  { id: 'padding', jaName: 'パディング', enName: 'Padding', description: '入力の周囲にゼロ等を追加する処理。出力サイズの維持や境界情報の保持に使用。', sectionId: 'dl-basic', subsectionId: 'dl-cnn' },
  { id: 'pooling', jaName: 'プーリング', enName: 'Pooling', description: '特徴マップのダウンサンプリング操作。空間的な不変性を獲得し計算量を削減する。', sectionId: 'dl-basic', subsectionId: 'dl-cnn' },
  { id: 'max-pooling', jaName: '最大プーリング', enName: 'Max Pooling', description: '領域内の最大値を出力するプーリング。最も顕著な特徴を保持する。', sectionId: 'dl-basic', subsectionId: 'dl-cnn' },
  { id: 'avg-pooling', jaName: '平均プーリング', enName: 'Average Pooling', description: '領域内の平均値を出力するプーリング。全体的な特徴の要約に適する。', sectionId: 'dl-basic', subsectionId: 'dl-cnn' },
  { id: 'global-avg-pooling', jaName: 'グローバル平均プーリング', enName: 'Global Average Pooling', description: '特徴マップ全体の平均を取るプーリング。全結合層の代わりに使用しパラメータを削減。', sectionId: 'dl-basic', subsectionId: 'dl-cnn' },
  { id: 'feature-map', jaName: '特徴マップ', enName: 'Feature Map', description: '畳み込み層の出力。入力画像から抽出された特徴の空間的な分布を表す。', sectionId: 'dl-basic', subsectionId: 'dl-cnn' },
  { id: 'receptive-field', jaName: '受容野', enName: 'Receptive Field', description: '出力の1つのニューロンが参照する入力の空間的範囲。層を重ねると受容野が拡大する。', sectionId: 'dl-basic', subsectionId: 'dl-cnn' },
  { id: 'depthwise-conv', jaName: 'Depthwise Convolution', enName: 'Depthwise Convolution', description: 'チャネルごとに独立して畳み込みを行う軽量な演算。MobileNetで使用される。', sectionId: 'dl-basic', subsectionId: 'dl-cnn' },
  { id: 'pointwise-conv', jaName: 'Pointwise Convolution', enName: 'Pointwise Convolution', description: '1x1の畳み込みでチャネル方向の情報を統合する演算。チャネル数の変換に使用。', sectionId: 'dl-basic', subsectionId: 'dl-cnn' },
  { id: 'dilated-conv', jaName: 'Dilated Convolution（膨張畳み込み）', enName: 'Dilated Convolution', description: 'フィルタ要素間に間隔を空けた畳み込み。パラメータを増やさず受容野を拡大できる。', sectionId: 'dl-basic', subsectionId: 'dl-cnn' },
  { id: 'deconv', jaName: '転置畳み込み', enName: 'Transposed Convolution', description: '特徴マップのアップサンプリングを行う演算。セグメンテーションや生成モデルで使用。', sectionId: 'dl-basic', subsectionId: 'dl-cnn' },
  // RNN
  { id: 'rnn', jaName: '再帰型ニューラルネットワーク', enName: 'Recurrent Neural Network', description: '時系列データを処理する隠れ状態を持つネットワーク。過去の情報を記憶して利用する。', sectionId: 'dl-basic', subsectionId: 'dl-rnn' },
  { id: 'lstm', jaName: 'LSTM', enName: 'Long Short-Term Memory', description: 'ゲート機構で長期依存関係を学習するRNN。忘却・入力・出力ゲートを持つ。', sectionId: 'dl-basic', subsectionId: 'dl-rnn' },
  { id: 'gru', jaName: 'GRU', enName: 'Gated Recurrent Unit', description: 'LSTMを簡略化した2ゲート（リセット・更新）のRNN。LSTMと同等の性能で計算効率が良い。', sectionId: 'dl-basic', subsectionId: 'dl-rnn' },
  { id: 'bptt', jaName: 'BPTT', enName: 'Backpropagation Through Time', description: 'RNNを時間方向に展開して誤差逆伝播を行う学習アルゴリズム。', sectionId: 'dl-basic', subsectionId: 'dl-rnn' },
  { id: 'bidirectional-rnn', jaName: '双方向RNN', enName: 'Bidirectional RNN', description: '順方向と逆方向の2つのRNNを用いて過去と未来の文脈を同時に捉える構造。', sectionId: 'dl-basic', subsectionId: 'dl-rnn' },
  { id: 'seq2seq', jaName: 'Sequence-to-Sequence', enName: 'Sequence-to-Sequence', description: '可変長の入力系列から可変長の出力系列を生成するモデル。機械翻訳の基礎。', sectionId: 'dl-basic', subsectionId: 'dl-rnn' },
  { id: 'encoder-decoder', jaName: 'エンコーダ・デコーダ', enName: 'Encoder-Decoder', description: '入力を固定長ベクトルに圧縮し、そこから出力を生成する構造。Seq2Seqの基本アーキテクチャ。', sectionId: 'dl-basic', subsectionId: 'dl-rnn' },
  { id: 'teacher-forcing', jaName: 'Teacher Forcing', enName: 'Teacher Forcing', description: 'デコーダの入力に前ステップの予測でなく正解を使用する学習手法。収束を高速化する。', sectionId: 'dl-basic', subsectionId: 'dl-rnn' },
  { id: 'beam-search', jaName: 'ビームサーチ', enName: 'Beam Search', description: '複数の候補系列を同時に探索するデコーディング手法。貪欲法より高品質な出力を得る。', sectionId: 'dl-basic', subsectionId: 'dl-rnn' },
  { id: 'hidden-state', jaName: '隠れ状態', enName: 'Hidden State', description: 'RNNの各時刻における内部状態ベクトル。過去の入力情報を圧縮して保持する。', sectionId: 'dl-basic', subsectionId: 'dl-rnn' },
  { id: 'forget-gate', jaName: '忘却ゲート', enName: 'Forget Gate', description: 'LSTMでセル状態のどの情報を忘れるかを制御するゲート。', sectionId: 'dl-basic', subsectionId: 'dl-rnn' },
  { id: 'cell-state', jaName: 'セル状態', enName: 'Cell State', description: 'LSTMの長期記憶を保持するベクトル。ゲート機構で選択的に情報が追加・削除される。', sectionId: 'dl-basic', subsectionId: 'dl-rnn' },
  // Transformer
  { id: 'self-attention', jaName: '自己注意機構', enName: 'Self-Attention', description: '系列内の各要素が他の全要素との関連度を計算する機構。Transformerの中核。', sectionId: 'dl-basic', subsectionId: 'dl-transformer' },
  { id: 'multi-head-attention', jaName: 'マルチヘッド注意', enName: 'Multi-Head Attention', description: '複数の注意ヘッドで異なる部分空間の情報を並列に捉える機構。表現力を向上させる。', sectionId: 'dl-basic', subsectionId: 'dl-transformer' },
  { id: 'scaled-dot-product', jaName: 'スケーリングドット積注意', enName: 'Scaled Dot-Product Attention', description: 'クエリとキーの内積をキー次元の平方根で割ったスケーリング付き注意計算。', sectionId: 'dl-basic', subsectionId: 'dl-transformer' },
  { id: 'positional-encoding', jaName: '位置エンコーディング', enName: 'Positional Encoding', description: '系列内の位置情報を入力埋め込みに加える手法。Transformerは順序を持たないため必要。', sectionId: 'dl-basic', subsectionId: 'dl-transformer' },
  { id: 'feed-forward', jaName: 'フィードフォワードネットワーク', enName: 'Feed-Forward Network', description: 'Transformerブロック内の全結合層。注意機構の出力を非線形変換する。', sectionId: 'dl-basic', subsectionId: 'dl-transformer' },
  { id: 'residual-connection', jaName: '残差結合', enName: 'Residual Connection', description: '層の入力を出力に直接加える接続。勾配の流れを改善し深い学習を可能にする。', sectionId: 'dl-basic', subsectionId: 'dl-transformer' },
  { id: 'layer-norm-transformer', jaName: 'レイヤー正規化（Transformer）', enName: 'Layer Normalization (in Transformer)', description: 'Transformerの各サブ層の出力を正規化する処理。Pre-LNとPost-LNの配置がある。', sectionId: 'dl-basic', subsectionId: 'dl-transformer' },
  { id: 'query-key-value', jaName: 'クエリ・キー・バリュー', enName: 'Query, Key, Value', description: '注意機構の3つの要素。クエリとキーの類似度でバリューの重み付けを行う。', sectionId: 'dl-basic', subsectionId: 'dl-transformer' },
  { id: 'bert', jaName: 'BERT', enName: 'Bidirectional Encoder Representations from Transformers', description: '双方向エンコーダによる事前学習モデル。マスク言語モデルと次文予測で学習する。', sectionId: 'dl-basic', subsectionId: 'dl-transformer' },
  { id: 'gpt', jaName: 'GPT', enName: 'Generative Pre-trained Transformer', description: '自己回帰型のデコーダベース言語モデル。大規模テキストで事前学習しテキスト生成に優れる。', sectionId: 'dl-basic', subsectionId: 'dl-transformer' },
  { id: 'vision-transformer', jaName: 'Vision Transformer', enName: 'Vision Transformer (ViT)', description: '画像をパッチに分割してTransformerで処理するモデル。大規模データで高い性能を発揮。', sectionId: 'dl-basic', subsectionId: 'dl-transformer' },
  { id: 'cross-attention', jaName: 'クロスアテンション', enName: 'Cross-Attention', description: '異なる系列間で注意を計算する機構。デコーダがエンコーダの出力を参照する際に使用。', sectionId: 'dl-basic', subsectionId: 'dl-transformer' },
  { id: 'masked-attention', jaName: 'マスク付き注意', enName: 'Masked Attention', description: '未来の位置への注意を遮断するマスクを適用した注意機構。自己回帰デコーダで使用。', sectionId: 'dl-basic', subsectionId: 'dl-transformer' },
  { id: 'flash-attention', jaName: 'Flash Attention', enName: 'Flash Attention', description: 'メモリ効率と計算速度を改善した注意計算アルゴリズム。IO回数を削減して高速化する。', sectionId: 'dl-basic', subsectionId: 'dl-transformer' },
  // 汎化性能
  { id: 'generalization', jaName: '汎化', enName: 'Generalization', description: '訓練データにない未知のデータに対しても正しく予測できる能力。', sectionId: 'dl-basic', subsectionId: 'dl-generalization' },
  { id: 'regularization-theory', jaName: '正則化理論', enName: 'Regularization Theory', description: '過学習を防ぎ汎化性能を向上させるための理論的枠組み。', sectionId: 'dl-basic', subsectionId: 'dl-generalization' },
  { id: 'double-descent', jaName: 'ダブルディセント', enName: 'Double Descent', description: 'モデルの複雑さを増すとテスト誤差が一度上がった後に再び下がる現象。', sectionId: 'dl-basic', subsectionId: 'dl-generalization' },
  { id: 'lottery-ticket', jaName: '宝くじ仮説', enName: 'Lottery Ticket Hypothesis', description: '密なNNには初期化時に良い性能を出せる疎なサブネットワーク（当たりくじ）が存在するという仮説。', sectionId: 'dl-basic', subsectionId: 'dl-generalization' },
  { id: 'neural-scaling-law', jaName: 'ニューラルスケーリング則', enName: 'Neural Scaling Law', description: 'モデルサイズ・データ量・計算量と性能のべき乗則的な関係。', sectionId: 'dl-basic', subsectionId: 'dl-generalization' },
  { id: 'flat-minima', jaName: '平坦な最小値', enName: 'Flat Minima', description: '損失曲面の平坦な領域にある最小値。鋭い最小値より汎化性能が高いとされる。', sectionId: 'dl-basic', subsectionId: 'dl-generalization' },
  { id: 'sharpness-aware', jaName: 'Sharpness-Aware Minimization', enName: 'Sharpness-Aware Minimization (SAM)', description: '損失曲面の鋭さを同時に最小化する最適化手法。平坦な最小値への収束を促す。', sectionId: 'dl-basic', subsectionId: 'dl-generalization' },
  { id: 'knowledge-distillation-gen', jaName: '知識蒸留（汎化）', enName: 'Knowledge Distillation', description: '大きなモデルの知識を小さなモデルに転写する手法。汎化性能の向上にも寄与する。', sectionId: 'dl-basic', subsectionId: 'dl-generalization' },
];
