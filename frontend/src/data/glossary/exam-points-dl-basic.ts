import type { TermExamPoints } from '@/types/glossary';

/**
 * E資格 深層学習基礎 試験ポイント
 * 各用語に対する出題パターン・頻出公式・間違えやすいポイントを記載
 */
export const DL_BASIC_EXAM_POINTS: TermExamPoints[] = [
  // ============================
  // 順伝播型NN (dl-ffnn)
  // ============================
  {
    termId: 'perceptron',
    points: [
      '単純パーセプトロンはXOR問題を解けないことが頻出。線形分離不可能な問題には多層が必要。',
      '活性化関数にステップ関数を用い、出力は0か1の二値であることを押さえる。',
      'パーセプトロンの学習規則（重み更新式）とデルタ則の違いを区別できるようにする。',
    ],
  },
  {
    termId: 'mlp',
    points: [
      '隠れ層が1つ以上あれば非線形分離が可能になる点が出題される。',
      '全結合層の計算（行列積＋バイアス＋活性化関数）を手計算できるようにする。',
      'パラメータ数の計算問題が頻出。入力次元×出力次元＋バイアス数を正確に数えること。',
    ],
  },
  {
    termId: 'forward-prop',
    points: [
      '各層の出力 h = f(Wx + b) の計算を具体的な数値で追えるようにする。',
      '行列の次元が正しく対応しているか（入力ベクトル・重み行列・出力ベクトル）を確認する問題が出る。',
      '推論時とは異なり、学習時はDropout等が有効になる点に注意。',
    ],
    formula: 'z = Wx + b, a = f(z)',
  },
  {
    termId: 'backprop',
    points: [
      '計算グラフ上での勾配の逆向き伝播を手計算で追う問題が頻出。',
      '各ノードでの局所勾配と上流勾配の積で求まることを理解する。',
      '数値微分との違い（計算効率）を聞かれることがある。',
      '活性化関数の微分を正しく求められるようにしておく。',
    ],
  },
  {
    termId: 'gradient',
    points: [
      '勾配ベクトルは損失関数が最も急に増加する方向を指すことを理解する。',
      '勾配降下法では勾配の逆方向にパラメータを更新する。更新式 w ← w - η∇L を覚える。',
      '数値微分（中心差分法）の公式も聞かれることがある。',
    ],
    formula: '∇L = (∂L/∂w₁, ∂L/∂w₂, ..., ∂L/∂wₙ)',
  },
  {
    termId: 'chain-rule',
    points: [
      '合成関数 f(g(x)) の微分 = f\'(g(x)) × g\'(x) を確実に適用できるようにする。',
      '多変数の場合の連鎖律（偏微分の連鎖）が誤差逆伝播の数学的基盤であることを理解する。',
      '計算グラフでの加算ノード（勾配をそのまま流す）と乗算ノード（値を入れ替えて掛ける）の違いを押さえる。',
    ],
    formula: '∂L/∂x = (∂L/∂y)(∂y/∂x)',
  },
  {
    termId: 'computational-graph',
    points: [
      '加算・乗算・活性化関数などの基本ノードの局所勾配を即答できるようにする。',
      '順伝播で各ノードの値を求め、逆伝播で勾配を求める問題が定番。',
      'ReLUノードの逆伝播は入力が正なら勾配をそのまま、負なら0にすることを覚える。',
    ],
  },
  {
    termId: 'hidden-layer',
    points: [
      '層の数と幅がモデルの表現力に影響する。深さ（層数）は特徴の抽象度に対応する。',
      '隠れ層の出力次元を変えるとパラメータ数がどう変化するか計算問題で問われる。',
    ],
  },
  {
    termId: 'neuron',
    points: [
      'ニューロンの計算は「重み付き和→活性化関数」の2ステップであることを明確に理解する。',
      '生物学的ニューロンとの対応（樹状突起＝入力、軸索＝出力）を聞かれることがある。',
    ],
    formula: 'y = f(Σ wᵢxᵢ + b)',
  },
  {
    termId: 'weight',
    points: [
      '重みの初期値が学習に大きく影響する。すべて同じ値に初期化すると対称性の問題が発生する。',
      '重み行列の各要素が対応する入力と出力のニューロン間の結合強度を表すことを理解する。',
    ],
  },
  {
    termId: 'bias',
    points: [
      'バイアスがないと活性化関数への入力が原点を通る超平面に限定され、表現力が下がる。',
      'バイアスは通常0で初期化しても問題ないが、重みを0で初期化するのはNGという違いを押さえる。',
    ],
  },
  {
    termId: 'universal-approx',
    points: [
      '「十分な幅の隠れ層1つ」で任意の連続関数を近似可能だが、必要なニューロン数は非常に多い場合がある。',
      '存在定理であり、学習で到達できるかは保証しない点に注意。',
      '深さ（層数）を増やすことで指数的に効率よく表現できる場合がある点も問われる。',
    ],
  },

  // ============================
  // 活性化関数 (dl-activation)
  // ============================
  {
    termId: 'sigmoid',
    points: [
      '出力範囲は(0, 1)。σ(0) = 0.5 であることを即答できるようにする。',
      '微分が σ(x)(1 - σ(x)) であることが頻出。最大値は0.25（x=0のとき）。',
      '入力が大きい/小さいとき勾配が0に近づく（勾配消失問題）点を理解する。',
      '二値分類の出力層で使用される場面が多いが、隠れ層ではReLU系が推奨される理由を押さえる。',
    ],
    formula: 'σ(x) = 1 / (1 + e^(-x))',
  },
  {
    termId: 'tanh',
    points: [
      '出力範囲は(-1, 1)で原点対称。sigmoidの線形変換 tanh(x) = 2σ(2x) - 1 の関係を覚える。',
      '微分が 1 - tanh²(x) であることが出題される。',
      'sigmoidより勾配消失が軽減されるが、完全には解消されない。',
    ],
    formula: 'tanh(x) = (e^x - e^(-x)) / (e^x + e^(-x))',
  },
  {
    termId: 'relu',
    points: [
      'x > 0 なら出力はx、x ≤ 0 なら出力は0。微分は x > 0 で1、x < 0 で0。',
      '計算が非常に軽く、勾配消失が起きにくいため隠れ層で最も広く使われる。',
      'x = 0 での微分が未定義だが、実装では0または1として扱う。',
      'Dying ReLU問題（負の入力が続くとニューロンが死ぬ）を理解すること。',
    ],
    formula: 'f(x) = max(0, x)',
  },
  {
    termId: 'leaky-relu',
    points: [
      '負の領域に小さな傾きα（通常0.01）を持たせることでDying ReLU問題を緩和する。',
      '微分は x > 0 で1、x < 0 でαとなり、負の領域でも勾配が伝わる。',
      'αの値が固定である点がPReLUとの違い。',
    ],
    formula: 'f(x) = max(αx, x), α ≈ 0.01',
  },
  {
    termId: 'prelu',
    points: [
      '負の領域の傾きαを学習可能なパラメータとした点がLeaky ReLUとの違い。',
      'αが学習されるため、データに適応した活性化関数となる。',
    ],
    formula: 'f(x) = max(αx, x), αは学習パラメータ',
  },
  {
    termId: 'elu',
    points: [
      '負の入力で指数関数を用いるため、出力の平均が0に近づきやすい利点がある。',
      'x < 0 のとき α(e^x - 1) で定義され、x→-∞で-αに飽和する。',
      'ReLUとの違いとして、負の領域でも微分が0にならない点を理解する。',
    ],
    formula: 'f(x) = x (x > 0), α(e^x - 1) (x ≤ 0)',
  },
  {
    termId: 'gelu',
    points: [
      'TransformerやBERTで標準採用。ガウス分布の累積分布関数(CDF)に基づく。',
      '近似式 0.5x(1 + tanh(√(2/π)(x + 0.044715x³))) が使われることが多い。',
      'ReLUと似ているが滑らかで、確率的にニューロンの出力をゲーティングする解釈がある。',
    ],
    formula: 'GELU(x) = x · Φ(x), Φはガウス分布のCDF',
  },
  {
    termId: 'swish',
    points: [
      'x × sigmoid(βx) で定義される。β=1のとき SiLU と同義。',
      'ReLUより滑らかで、非単調な性質を持つ（x < 0で負の値を取りうる）。',
      'Google Brainの研究で自動探索により発見された活性化関数。',
    ],
    formula: 'f(x) = x · σ(βx)',
  },
  {
    termId: 'softmax',
    points: [
      '出力の総和が1になり確率分布として解釈できる。多クラス分類の出力層で必須。',
      '数値安定性のため、入力の最大値を引く実装テクニック（max引き）が頻出。',
      '温度パラメータTで分布のシャープさを制御できることも問われる。',
      '交差エントロピー損失との組み合わせで勾配が y - t（予測-正解）になる点が重要。',
    ],
    formula: 'softmax(xᵢ) = e^(xᵢ) / Σⱼ e^(xⱼ)',
  },
  {
    termId: 'softplus',
    points: [
      'ReLUの滑らかな近似として理解する。x→∞でxに、x→-∞で0に近づく。',
      '微分がsigmoid関数σ(x)になることが出題のポイント。',
    ],
    formula: 'f(x) = log(1 + e^x)',
  },
  {
    termId: 'mish',
    points: [
      'x × tanh(softplus(x))で定義される自己正則化関数。滑らかで非単調。',
      'YOLOv4等の物体検出モデルで採用され、ReLUやSwishより良い結果を出すケースがある。',
    ],
    formula: 'f(x) = x · tanh(ln(1 + e^x))',
  },
  {
    termId: 'dying-relu',
    points: [
      '大きな負の勾配が流れるとReLUニューロンが恒久的に0を出力し続ける問題。',
      '対策としてLeaky ReLU, PReLU, ELU等が使われる点を関連付けて覚える。',
      '学習率が大きすぎる場合に発生しやすいことも押さえる。',
    ],
  },

  // ============================
  // 損失関数 (dl-loss)
  // ============================
  {
    termId: 'mse-loss',
    points: [
      '回帰タスクの標準損失。外れ値に対して二乗で大きなペナルティを与える。',
      '微分が (予測値 - 正解値) に比例する点を理解する。',
      'MAE（平均絶対誤差）との違い（外れ値への感度）を比較問題で問われる。',
    ],
    formula: 'L = (1/n) Σ (yᵢ - ŷᵢ)²',
  },
  {
    termId: 'cross-entropy-loss',
    points: [
      '多クラス分類の標準損失。softmax出力と組み合わせて使用する。',
      '正解クラスの予測確率が低いほど損失が大きくなる（-logの性質）。',
      'softmaxとの合成微分が (予測確率 - 正解ラベル) になる点が計算問題で重要。',
    ],
    formula: 'L = -Σ tᵢ log(yᵢ)',
  },
  {
    termId: 'binary-cross-entropy',
    points: [
      '二値分類ではsigmoid + BCEの組み合わせ。正例・負例の両方のロスを加算する。',
      'マルチラベル分類にも使用できる点がsoftmax+CEとの違い。',
      '出力が0や1に近づくとlogが発散するため、クリッピング処理が必要な点に注意。',
    ],
    formula: 'L = -[t log(y) + (1-t) log(1-y)]',
  },
  {
    termId: 'hinge-loss',
    points: [
      'SVMの損失関数。マージン1未満の場合にペナルティが発生する。',
      'max(0, 1 - t·y) の形式で、t ∈ {-1, +1} であることに注意。',
      '微分不可能な点があるがサブグラデントで対応可能。',
    ],
    formula: 'L = max(0, 1 - t · y)',
  },
  {
    termId: 'huber-loss',
    points: [
      '|誤差| ≤ δ でMSE、|誤差| > δ でMAEのように振る舞い、外れ値にロバスト。',
      'δの設定が重要で、δ→∞でMSE、δ→0でMAEに近づく。',
      '物体検出のバウンディングボックス回帰でよく使用される。',
    ],
    formula: 'L = 0.5(y-ŷ)² (|y-ŷ|≤δ), δ|y-ŷ|-0.5δ² (otherwise)',
  },
  {
    termId: 'focal-loss',
    points: [
      '交差エントロピーに (1-pₜ)^γ の重みを掛けて簡単なサンプルの影響を低減する。',
      'γ（ガンマ）が大きいほど簡単なサンプルの損失をより強く抑制する。',
      '物体検出（RetinaNet）でのクラス不均衡問題に対処するために提案された。',
    ],
    formula: 'FL(pₜ) = -αₜ(1 - pₜ)^γ log(pₜ)',
  },
  {
    termId: 'triplet-loss',
    points: [
      'anchor-positive間距離がanchor-negative間距離よりマージンα以上小さくなるように学習する。',
      'ハードネガティブマイニング（難しい負例を選ぶ）がトリプレット学習の性能に大きく影響する。',
      '顔認識（FaceNet）や類似度学習で頻出のトピック。',
    ],
    formula: 'L = max(0, d(a,p) - d(a,n) + α)',
  },
  {
    termId: 'contrastive-loss',
    points: [
      '類似ペア（y=1）では距離を最小化、非類似ペア（y=0）ではマージンm以上の距離を確保する。',
      'SimCLR等の自己教師あり学習で発展的なコントラスティブ損失が使われる。',
    ],
    formula: 'L = y·d² + (1-y)·max(0, m-d)²',
  },
  {
    termId: 'kl-loss',
    points: [
      'KLダイバージェンスは非対称（D_KL(P||Q) ≠ D_KL(Q||P)）であることが頻出。',
      'VAEの損失関数でデコーダの再構成誤差＋KLダイバージェンスの構成を理解する。',
      'D_KL ≥ 0（ギブスの不等式）で、P=Qのとき0になる。',
    ],
    formula: 'D_KL(P||Q) = Σ P(x) log(P(x)/Q(x))',
  },
  {
    termId: 'label-smoothing',
    points: [
      '正解ラベルを1ではなく (1-ε) とし、他クラスに ε/(K-1) を分配する手法。',
      'モデルが正解クラスに過度に自信を持つ（過学習）のを防ぐ効果がある。',
      'εの典型的な値は0.1程度。Transformerの学習で標準的に使用される。',
    ],
  },

  // ============================
  // 最適化 (dl-optim)
  // ============================
  {
    termId: 'sgd',
    points: [
      '更新式 w ← w - η∇L をベースに、バッチ・ミニバッチ・確率的の3種類を区別する。',
      'ミニバッチSGDが最も一般的。バッチサイズが汎化性能と学習速度に影響する。',
      '学習率ηが大きすぎると発散、小さすぎると収束が遅い問題を理解する。',
    ],
    formula: 'w ← w - η∇L(w)',
  },
  {
    termId: 'momentum',
    points: [
      '速度項vを導入し、勾配の移動平均で更新方向を安定化する。',
      '慣性係数μ（通常0.9）により、過去の勾配の情報が蓄積される。',
      '谷底の振動を抑え、勾配が一貫している方向への移動を加速する効果がある。',
    ],
    formula: 'v ← μv - η∇L, w ← w + v',
  },
  {
    termId: 'nesterov',
    points: [
      'モメンタムとの違いは、先読み位置(w + μv)で勾配を計算する点。',
      '先読みにより「行き過ぎ」を検知して減速でき、モメンタムより速い収束が可能。',
      '実装では等価な変形を行い、現在位置での勾配計算に帰着させることが多い。',
    ],
    formula: 'v ← μv - η∇L(w + μv), w ← w + v',
  },
  {
    termId: 'adagrad',
    points: [
      '各パラメータの勾配の二乗和を蓄積し、更新が多いパラメータの学習率を下げる。',
      '問題点は学習が進むと分母が大きくなりすぎて学習が停止すること。RMSPropで改善。',
      'スパースな勾配を持つ問題（NLP等）で効果的。',
    ],
    formula: 'G ← G + (∇L)², w ← w - η∇L / √(G + ε)',
  },
  {
    termId: 'rmsprop',
    points: [
      'AdaGradの改良で、勾配の二乗を指数移動平均で計算し学習率減衰を緩和する。',
      '減衰率ρ（通常0.9）がAdaGradとの最大の違い。古い勾配情報を忘却する。',
      'RNNの学習でよく使われた歴史的背景（Hintonの講義スライドが初出）を押さえる。',
    ],
    formula: 'E[g²] ← ρE[g²] + (1-ρ)(∇L)², w ← w - η∇L / √(E[g²] + ε)',
  },
  {
    termId: 'adam',
    points: [
      'モメンタム（一次モーメント）とRMSProp（二次モーメント）の統合。最も広く使われる最適化手法。',
      'バイアス補正（m̂ = m/(1-β₁ᵗ), v̂ = v/(1-β₂ᵗ)）が初期ステップの不安定さを解消する。',
      'デフォルトの β₁=0.9, β₂=0.999, ε=1e-8 を覚えておく。',
      'L2正則化との組み合わせで重み減衰が正しく機能しない問題がAdamWで解決される。',
    ],
    formula: 'm ← β₁m + (1-β₁)∇L, v ← β₂v + (1-β₂)(∇L)², w ← w - η·m̂/√(v̂+ε)',
  },
  {
    termId: 'adamw',
    points: [
      'Adamの更新式に重み減衰を直接加える（L2正則化とは異なる）点が重要。',
      'Transformerの学習ではAdamWが事実上のデファクトスタンダード。',
      'Adamでは適応学習率が重み減衰の効果を打ち消す問題があり、AdamWで分離して解決した。',
    ],
    formula: 'w ← w - η(m̂/√(v̂+ε) + λw)',
  },
  {
    termId: 'learning-rate',
    points: [
      '最も重要なハイパーパラメータの一つ。大きすぎると発散、小さすぎると収束が遅い。',
      '適切な学習率を見つけるためLR range testやグリッドサーチが用いられる。',
      'モデルやバッチサイズによって最適な学習率が異なる。バッチサイズを大きくすると学習率も大きくする必要がある。',
    ],
  },
  {
    termId: 'lr-schedule',
    points: [
      'Step Decay: 一定エポックごとに学習率を1/10等にする最もシンプルな方法。',
      'Cosine Annealing: 学習率をコサイン曲線で徐々に減衰させる。Transformer学習で多用。',
      '各スケジュールの挙動の違い（急激 vs 緩やか、再上昇の有無）をグラフで理解する。',
    ],
  },
  {
    termId: 'warmup',
    points: [
      '学習初期に学習率を0から徐々に上げることで、ランダムな初期重みでの大きな更新を防ぐ。',
      'Transformerでは線形ウォームアップ後にコサインデケイするスケジュールが定番。',
      'ウォームアップステップ数はモデルサイズやデータ量で決定する。',
    ],
  },
  {
    termId: 'gradient-clipping',
    points: [
      'ノルムクリッピング（勾配全体のL2ノルムを閾値で制限）と値クリッピング（各要素を制限）の2種類がある。',
      'RNNの学習で勾配爆発を防ぐために必須の技術。',
      '閾値の典型的な値は1.0〜5.0程度。',
    ],
  },
  {
    termId: 'vanishing-gradient',
    points: [
      'sigmoid/tanhの微分最大値が1未満のため、層が深いと勾配が指数的に減衰する。',
      '対策としてReLU、残差接続、LSTM/GRUのゲート機構、適切な初期化が重要。',
      'BatchNormも勾配消失の緩和に効果があることを理解する。',
    ],
  },
  {
    termId: 'exploding-gradient',
    points: [
      '勾配が指数的に増大し、パラメータ更新が極端に大きくなる問題。NaN発生の原因。',
      '勾配クリッピングが最も直接的な対策。重み初期化やBatchNormも有効。',
      'RNNで特に発生しやすく、BPTT（時間方向の展開）で顕在化する。',
    ],
  },
  {
    termId: 'batch-size',
    points: [
      '大きいバッチ: 勾配の推定が安定、GPU効率が良いが汎化性能が下がる傾向。',
      '小さいバッチ: ノイズの多い勾配推定が正則化効果を持ち、汎化性能が良い傾向。',
      'バッチサイズと学習率の線形スケーリング則を理解する。',
    ],
  },

  // ============================
  // 重み初期化 (dl-init)
  // ============================
  {
    termId: 'xavier-init',
    points: [
      'sigmoid/tanhで使用する初期化。分散を 1/n_in（fan-in）または 2/(n_in+n_out) に設定。',
      '活性化関数とのセットで出題される（Xavier→sigmoid/tanh, He→ReLU）。',
      'Glorot初期化とも呼ばれる。正規分布版と一様分布版がある。',
    ],
    formula: 'Var(w) = 2 / (n_in + n_out)',
  },
  {
    termId: 'he-init',
    points: [
      'ReLU活性化関数に最適化された初期化手法。分散を 2/n_in に設定する。',
      'ReLUが負の入力を0にするため、Xavierの2倍の分散が必要な直感を理解する。',
      'Kaiming初期化とも呼ばれる。ReLUの変種（Leaky ReLU等）にも対応可能。',
    ],
    formula: 'Var(w) = 2 / n_in',
  },
  {
    termId: 'lecun-init',
    points: [
      'SELU活性化関数と組み合わせて自己正規化ネットワーク(SNN)を構成する。',
      '分散を 1/n_in に設定する。Xavier初期化のfan-in版に相当。',
    ],
    formula: 'Var(w) = 1 / n_in',
  },
  {
    termId: 'zero-init',
    points: [
      '全重みを0にすると対称性の問題で全ニューロンが同一の勾配を受け学習が進まない。',
      'バイアスの初期化には0が使われることが多い。重みには使ってはいけない理由を説明できるようにする。',
      '残差接続の最終層を0初期化するテクニック（Zero Init Residual）は例外的に有効。',
    ],
  },
  {
    termId: 'random-init',
    points: [
      '対称性の破壊が目的。値が大きすぎると勾配爆発、小さすぎると勾配消失を引き起こす。',
      '現在はXavierやHe初期化など、活性化関数に適した手法が使われるのが標準。',
    ],
  },
  {
    termId: 'orthogonal-init',
    points: [
      '直交行列は特異値がすべて1であり、信号の増幅も減衰もしない性質を持つ。',
      'RNNの隠れ状態間の重み行列に適用すると、長期依存関係の学習が安定する。',
    ],
  },

  // ============================
  // 正則化 (dl-reg)
  // ============================
  {
    termId: 'l1-reg',
    points: [
      '損失関数に λΣ|wᵢ| を加える。重みを0に押しやりスパース（疎）な解を得やすい。',
      'Lasso回帰で使われる。特徴選択の効果がある点がL2との違い。',
      '微分が不連続（0で微分不可能）だが、サブグラデントで対処する。',
    ],
    formula: 'L_total = L + λΣ|wᵢ|',
  },
  {
    termId: 'l2-reg',
    points: [
      '損失関数に (λ/2)Σwᵢ² を加える。重みを小さく保つが完全に0にはしにくい。',
      'Ridge回帰に対応する。すべてのパラメータに均等にペナルティを与える。',
      'SGDでの重み減衰と数学的に等価になるが、Adamでは等価にならない点がAdamWの動機。',
    ],
    formula: 'L_total = L + (λ/2)Σwᵢ²',
  },
  {
    termId: 'dropout',
    points: [
      '学習時に確率pでニューロンを無効化（出力を0に）し、テスト時は全ニューロンを使い出力を(1-p)倍する。',
      '逆ドロップアウト（学習時に1/(1-p)倍してテスト時はそのまま）の実装が主流。',
      'アンサンブル学習の近似と解釈できる点が出題される。',
      'CNNではSpatial Dropout、RNNではVariational Dropoutなどの変種がある。',
    ],
  },
  {
    termId: 'batch-norm',
    points: [
      'ミニバッチ内の平均と分散で正規化後、学習可能なγ（スケール）とβ（シフト）で変換する。',
      'テスト時は学習時に計算した移動平均の統計量を使う点が頻出。',
      '内部共変量シフトの軽減が元の動機だが、損失曲面の平滑化効果が真の理由とする説もある。',
      'バッチサイズが小さいと統計量の推定が不安定になるデメリットを理解する。',
    ],
    formula: 'y = γ · (x - μ_B) / √(σ²_B + ε) + β',
  },
  {
    termId: 'layer-norm',
    points: [
      'バッチ方向ではなく特徴方向に正規化する。バッチサイズに依存しない。',
      'Transformerで標準的に使用される。Pre-LN（層の前）とPost-LN（層の後）の配置の違いを理解する。',
      'RNNにも適用可能で、BatchNormが適用しにくい系列モデルに有効。',
    ],
  },
  {
    termId: 'group-norm',
    points: [
      'チャネルをG個のグループに分割し、グループ内で正規化する。',
      'G=1でLayer Norm、G=C（チャネル数）でInstance Normに相当する。',
      'バッチサイズが小さい場合にBatch Normより安定する利点がある。',
    ],
  },
  {
    termId: 'instance-norm',
    points: [
      '各サンプルの各チャネルごとに独立して正規化する。バッチ情報を使わない。',
      'スタイル変換（Neural Style Transfer）で標準的に使用される。',
      'コンテンツの統計量を正規化することでスタイル情報を取り除く効果がある。',
    ],
  },
  {
    termId: 'weight-decay',
    points: [
      '各ステップで重みを (1 - ηλ) 倍に縮小する正則化手法。',
      'SGDではL2正則化と数学的に等価だが、Adam等の適応的手法では等価にならない。',
      'この違いがAdamWの動機であり、試験で頻出のトピック。',
    ],
  },
  {
    termId: 'spectral-norm',
    points: [
      '重み行列の最大特異値（スペクトルノルム）を1に制約し、リプシッツ定数を制御する。',
      'GANのDiscriminatorの学習安定化に効果的（SNGAN）。',
      'べき乗法で効率的にスペクトルノルムを近似計算する。',
    ],
  },
  {
    termId: 'mixup',
    points: [
      '2つの訓練サンプルをλで線形補間。λはBeta分布 Beta(α, α) からサンプリングする。',
      '入力だけでなくラベルも補間する点が単純なデータ拡張との違い。',
      'モデルの決定境界を滑らかにし、汎化性能を向上させる。',
    ],
    formula: 'x̃ = λxᵢ + (1-λ)xⱼ, ỹ = λyᵢ + (1-λ)yⱼ',
  },
  {
    termId: 'cutout',
    points: [
      '画像のランダムな矩形領域を0（黒）でマスクする。遮蔽に対する頑健性が向上。',
      'ラベルは変更しない点がCutMixとの違い。',
      '実装が非常にシンプルで、CNNの汎化性能を手軽に向上できる。',
    ],
  },
  {
    termId: 'cutmix',
    points: [
      '画像の一部を別画像で置き換え、ラベルも面積比で混合する。CutoutとMixupの利点を統合。',
      'Cutoutと違い置き換えた領域にも意味のある情報があるため、より効率的に学習できる。',
      'λはBeta分布からサンプリングし、切り取る領域の面積比を決定する。',
    ],
  },

  // ============================
  // CNN (dl-cnn)
  // ============================
  {
    termId: 'convolution',
    points: [
      '出力サイズの計算公式 (W - F + 2P) / S + 1 は必ず覚えること。最頻出の計算問題。',
      '重み共有により全結合層と比べてパラメータ数が大幅に削減される利点を理解する。',
      '畳み込みは厳密には相互相関（cross-correlation）であり、数学的な畳み込みとはフィルタの反転有無が異なる。',
    ],
    formula: '出力サイズ = (W - F + 2P) / S + 1',
  },
  {
    termId: 'filter-kernel',
    points: [
      'フィルタ数が出力チャネル数を決定する。1つのフィルタで1つの特徴マップを出力する。',
      'フィルタのサイズは3×3が最も一般的。VGGNetで3×3フィルタの有効性が示された。',
      'パラメータ数の計算: フィルタ数 × (カーネルサイズ² × 入力チャネル数 + 1[バイアス])。',
    ],
  },
  {
    termId: 'stride',
    points: [
      'ストライドを大きくすると出力サイズが小さくなり、計算量が減る。',
      'ストライド2の畳み込みがプーリングの代替としてダウンサンプリングに使われることがある。',
      '出力サイズ計算で割り切れない場合の床関数（切り捨て）に注意。',
    ],
  },
  {
    termId: 'padding',
    points: [
      'Same Padding: 出力サイズを入力と同じに保つ。P = (F-1)/2 で設定する。',
      'Valid Padding: パディングなし。出力サイズは (W - F)/S + 1 で小さくなる。',
      'ゼロパディングが最も一般的だが、反射パディング等の変種もある。',
    ],
  },
  {
    termId: 'pooling',
    points: [
      '学習パラメータがないことが畳み込みとの違い。空間的な位置ずれへの不変性を獲得する。',
      '最大プーリングと平均プーリングの特性の違いを理解する。',
      'ストライド付き畳み込みに置き換えられるケースが増えている近年のトレンドも押さえる。',
    ],
  },
  {
    termId: 'max-pooling',
    points: [
      '領域内で最も大きい値（最も顕著な特徴）を保持する。分類タスクで一般的。',
      '逆伝播では最大値を取った位置にのみ勾配が流れ、他は0になる。',
    ],
  },
  {
    termId: 'avg-pooling',
    points: [
      '領域内の平均値を取ることで全体的な特徴を要約する。',
      '逆伝播では勾配が領域内の全要素に均等に分配される。',
      '最大プーリングより情報の損失が少ないが、顕著な特徴を薄めるデメリットもある。',
    ],
  },
  {
    termId: 'global-avg-pooling',
    points: [
      '特徴マップ全体を1つの値に要約し、全結合層の代わりに使用する（NiN, GoogLeNet等）。',
      'パラメータ数を大幅に削減し、過学習を抑制する効果がある。',
      '出力は (チャネル数, 1, 1) の形状になる。',
    ],
  },
  {
    termId: 'feature-map',
    points: [
      '浅い層ではエッジや色などの低レベル特徴、深い層では物体の部品など高レベル特徴を捉える。',
      'チャネル数は層が深くなるにつれ増加し、空間解像度は減少するのが典型的な設計。',
    ],
  },
  {
    termId: 'receptive-field',
    points: [
      '層を重ねるほど受容野が拡大する。3×3フィルタ2層で5×5フィルタ1層と同じ受容野を持つ。',
      '受容野の計算方法（各層のカーネルサイズとストライドから逆算）を理解する。',
      '小さなフィルタを重ねるほうが大きなフィルタより少ないパラメータで同じ受容野を得られる。',
    ],
  },
  {
    termId: 'depthwise-conv',
    points: [
      '各入力チャネルに独立したフィルタを適用する。チャネル間の情報統合は行わない。',
      'Pointwise Convと組み合わせてDepthwise Separable Convを構成する（MobileNet）。',
      '通常の畳み込みと比べてパラメータ数と計算量が大幅に削減される（約1/k²倍）。',
    ],
  },
  {
    termId: 'pointwise-conv',
    points: [
      '1×1畳み込みでチャネル方向の線形結合を行う。空間方向の情報は変えない。',
      'チャネル数の変換（増減）やDepthwise Convとの組み合わせで使用される。',
      'Network in Network (NiN)で提案され、GoogLeNetのInceptionモジュールでも使用。',
    ],
  },
  {
    termId: 'dilated-conv',
    points: [
      'フィルタ要素間にdilation rate-1の間隔を空け、パラメータを増やさず受容野を拡大する。',
      'dilation rate r のとき、実効的なフィルタサイズは k + (k-1)(r-1) になる。',
      '音声合成（WaveNet）やセマンティックセグメンテーション（DeepLab）で使用される。',
    ],
  },
  {
    termId: 'deconv',
    points: [
      '転置畳み込み（Transposed Convolution）が正式名称。「逆畳み込み」は不正確なので注意。',
      '出力サイズの計算: (入力-1)×ストライド - 2×パディング + カーネルサイズ。',
      'チェッカーボードアーティファクトが発生する問題があり、リサイズ＋畳み込みで代替されることもある。',
    ],
  },

  // ============================
  // RNN (dl-rnn)
  // ============================
  {
    termId: 'rnn',
    points: [
      '隠れ状態 h_t = f(W_hh · h_{t-1} + W_xh · x_t + b) の更新式を理解する。',
      '時間ステップが長いと勾配消失/爆発が発生する問題がLSTM/GRUの動機。',
      '同じ重み行列を全時刻で共有するパラメータ共有の概念を押さえる。',
    ],
    formula: 'h_t = tanh(W_hh · h_{t-1} + W_xh · x_t + b)',
  },
  {
    termId: 'lstm',
    points: [
      '忘却ゲート・入力ゲート・出力ゲートの3つのゲートとセル状態の役割を正確に理解する。',
      '各ゲートはsigmoid関数で0〜1の値を出力し、情報の通過量を制御する。',
      'セル状態が「コンベアベルト」のように情報を運ぶ構造が長期依存を可能にする。',
      'パラメータ数はRNNの4倍（4つの重み行列）であることを計算できるようにする。',
    ],
    formula: 'f_t = σ(W_f·[h_{t-1}, x_t] + b_f), i_t = σ(W_i·[h_{t-1}, x_t] + b_i), C_t = f_t⊙C_{t-1} + i_t⊙tanh(W_C·[h_{t-1}, x_t] + b_C)',
  },
  {
    termId: 'gru',
    points: [
      'リセットゲートと更新ゲートの2つのゲートで構成。LSTMより構造がシンプル。',
      'セル状態を持たず、隠れ状態のみで長期・短期記憶を管理する。',
      'LSTMとの性能差は多くのタスクで小さいが、パラメータ数が少なく計算効率が良い。',
    ],
    formula: 'z_t = σ(W_z·[h_{t-1}, x_t]), r_t = σ(W_r·[h_{t-1}, x_t]), h_t = (1-z_t)⊙h_{t-1} + z_t⊙tanh(W·[r_t⊙h_{t-1}, x_t])',
  },
  {
    termId: 'bptt',
    points: [
      'RNNを時間方向に展開して通常の逆伝播を適用する手法。展開した計算グラフを理解する。',
      '長い系列では計算コストとメモリが膨大になるため、Truncated BPTTで系列を区切って学習する。',
      '時間方向の勾配消失/爆発の原因は、同じ重み行列の繰り返し乗算にある。',
    ],
  },
  {
    termId: 'bidirectional-rnn',
    points: [
      '順方向RNNと逆方向RNNの隠れ状態を結合して使用する。各時刻で過去と未来の文脈が利用可能。',
      'BERTの「双方向」とは原理が異なる点に注意（BERTはマスク言語モデルで双方向文脈を獲得）。',
      'リアルタイム処理には不向き（未来の入力が必要）な点を理解する。',
    ],
  },
  {
    termId: 'seq2seq',
    points: [
      'エンコーダが入力を固定長ベクトルに圧縮し、デコーダがそこから出力を生成する。',
      '入力と出力の系列長が異なってもよい点がMLPとの大きな違い。',
      '固定長ベクトルのボトルネック問題がAttention機構の導入につながった。',
    ],
  },
  {
    termId: 'encoder-decoder',
    points: [
      'エンコーダとデコーダを分離することで柔軟な構造が可能になる。',
      'Attention導入前は最終隠れ状態のみがデコーダに渡されるボトルネックがあった。',
      'Transformer以降もEncoder-Decoder構造は広く使われている（T5, BART等）。',
    ],
  },
  {
    termId: 'teacher-forcing',
    points: [
      '学習時にデコーダの入力に前ステップの「正解」を使用する手法。収束が速い。',
      '推論時は正解がないため自身の予測を使う。この学習と推論のギャップをexposure biasと呼ぶ。',
      'Scheduled Samplingで正解と予測の使用比率を徐々に変えてギャップを軽減する手法がある。',
    ],
  },
  {
    termId: 'beam-search',
    points: [
      'ビーム幅Bで上位B個の候補を保持しながら探索する。B=1は貪欲法に等しい。',
      'ビーム幅を大きくすると探索が網羅的になるが、計算コストが増加する。',
      '出力系列の長さで正規化しないと短い系列が有利になるバイアスがある。',
    ],
  },
  {
    termId: 'hidden-state',
    points: [
      '各時刻の入力と前時刻の隠れ状態から計算される内部表現ベクトル。',
      '固定サイズのベクトルに過去の情報を圧縮するため、長い系列では初期の情報が失われやすい。',
      '隠れ状態の次元数がRNNの容量を決定するハイパーパラメータ。',
    ],
  },
  {
    termId: 'forget-gate',
    points: [
      'セル状態のどの情報を忘れるかをsigmoid関数（0〜1）で制御する。',
      '忘却ゲートのバイアスを1で初期化することで学習初期のセル状態のリセットを防ぐテクニックが重要。',
      'ゲートの出力が1に近いと情報を保持、0に近いと忘却する。',
    ],
    formula: 'f_t = σ(W_f · [h_{t-1}, x_t] + b_f)',
  },
  {
    termId: 'cell-state',
    points: [
      'LSTMの長期記憶を担うベクトル。忘却ゲートと入力ゲートで選択的に情報が更新される。',
      'セル状態は加算的な更新（勾配がそのまま流れる）のため、勾配消失が起きにくい。',
      '隠れ状態とセル状態の違い（隠れ状態は出力ゲート経由の短期記憶）を明確にする。',
    ],
  },

  // ============================
  // Transformer (dl-transformer)
  // ============================
  {
    termId: 'self-attention',
    points: [
      '系列内の各トークンが他の全トークンとの関連度を計算する。計算量は系列長の二乗 O(n²)。',
      'CNNの局所的な受容野やRNNの逐次的処理と対比して、全要素間の直接的な依存関係を捉える利点を理解する。',
      '注意重みの可視化で「どの単語がどの単語に注目しているか」を解釈できる。',
    ],
  },
  {
    termId: 'multi-head-attention',
    points: [
      'Q,K,Vをh個のヘッドに分割し、各ヘッドで独立に注意計算を行い結合する。',
      'ヘッド数h=8、d_model=512のとき各ヘッドの次元は d_k = 512/8 = 64 の計算を押さえる。',
      '各ヘッドが異なる部分空間の関係性を学習できるため、単一ヘッドより表現力が高い。',
    ],
    formula: 'MultiHead(Q,K,V) = Concat(head₁,...,headₕ)W^O, headᵢ = Attention(QWᵢ^Q, KWᵢ^K, VWᵢ^V)',
  },
  {
    termId: 'scaled-dot-product',
    points: [
      'Q·K^Tの内積を√d_kで割るスケーリングが最重要ポイント。割らないと次元が大きいとき内積が巨大になりsoftmaxが飽和する。',
      '計算量はO(n²d)で、系列長nの二乗に比例する点が長系列での課題。',
      'softmaxの前にマスクを適用することで、特定の位置への注意を遮断できる。',
    ],
    formula: 'Attention(Q,K,V) = softmax(QK^T / √d_k)V',
  },
  {
    termId: 'positional-encoding',
    points: [
      'Transformerは構造的に順序情報を持たないため、位置エンコーディングで補う必要がある。',
      '元論文では正弦波関数（sinとcos）を使用。偶数次元にsin、奇数次元にcosを割り当てる。',
      '学習可能な位置埋め込みとの違い（固定 vs 学習、系列長の汎化性）を理解する。',
    ],
    formula: 'PE(pos,2i) = sin(pos/10000^(2i/d)), PE(pos,2i+1) = cos(pos/10000^(2i/d))',
  },
  {
    termId: 'feed-forward',
    points: [
      'Transformerブロック内の2層全結合NN。中間層の次元は通常d_modelの4倍(2048等)。',
      '活性化関数はReLUまたはGELUが使用される。各位置に独立に適用される。',
      'パラメータ数はd_model × d_ff × 2（2つの重み行列）で、Attention層より多いことが多い。',
    ],
    formula: 'FFN(x) = W₂ · ReLU(W₁x + b₁) + b₂',
  },
  {
    termId: 'residual-connection',
    points: [
      '入力xをサブ層の出力に加算: x + Sublayer(x)。勾配が直接流れるパスを提供する。',
      '層が深くても勾配消失が起きにくく、非常に深いネットワーク（100層以上）の学習を可能にする。',
      'ResNetで提案され、TransformerのAttentionとFFNの各サブ層で使用される。',
    ],
    formula: 'output = x + Sublayer(x)',
  },
  {
    termId: 'layer-norm-transformer',
    points: [
      'Post-LN: 残差結合の後にLayerNormを適用（元論文）。Pre-LN: 残差結合の前に適用。',
      'Pre-LNのほうが学習が安定し、ウォームアップが不要になる場合が多い。',
      'BatchNormではなくLayerNormを使う理由は、系列長やバッチサイズへの非依存性。',
    ],
  },
  {
    termId: 'query-key-value',
    points: [
      'Queryは「何を探しているか」、Keyは「何を持っているか」、Valueは「実際の情報」と解釈できる。',
      'Q, K, Vは同一の入力から異なる線形変換で生成される（自己注意の場合）。',
      '情報検索のアナロジー（Queryで検索→Keyとマッチング→対応するValueを取得）で理解する。',
    ],
  },
  {
    termId: 'bert',
    points: [
      'Masked Language Model（ランダムに15%のトークンをマスクし予測）とNext Sentence Predictionで事前学習する。',
      'エンコーダのみの構造で、双方向の文脈を捉えられる点がGPT（デコーダのみ）との違い。',
      'ファインチューニングで様々なNLPタスクに適用できるtransfer learningの代表例。',
      '[CLS]トークンの出力を文全体の表現として分類タスクに使用する。',
    ],
  },
  {
    termId: 'gpt',
    points: [
      '自己回帰型デコーダモデル。左から右への一方向の言語モデルで次の単語を予測する。',
      'BERTとの構造の違い（エンコーダ vs デコーダ、双方向 vs 一方向）を明確にする。',
      'スケーリング則に従い、モデルサイズ・データ量・計算量の増大で性能が向上する。',
    ],
  },
  {
    termId: 'vision-transformer',
    points: [
      '画像をパッチに分割し、各パッチを線形射影でトークン化してTransformerに入力する。',
      '大規模データセット（JFT-300M等）での事前学習が高性能の鍵。少量データではCNNに劣る。',
      '[CLS]トークンを用いた分類ヘッドの構造を理解する。',
    ],
  },
  {
    termId: 'cross-attention',
    points: [
      'QueryはデコーダからKeyとValueはエンコーダから供給される点が自己注意との違い。',
      'Encoder-Decoder構造で、デコーダがエンコーダの情報を参照するために使用される。',
      '画像キャプション生成（画像→テキスト）等、異なるモダリティ間の情報統合にも使用される。',
    ],
  },
  {
    termId: 'masked-attention',
    points: [
      'デコーダの自己回帰性を保つため、未来の位置への注意を-∞（softmax後に0）でマスクする。',
      'GPT等のデコーダモデルで使用。学習時にすべての位置を並列に計算できる利点がある。',
      '上三角マスク行列の形状を理解する。マスク位置にはsoftmax前に非常に大きな負の値を加算する。',
    ],
  },
  {
    termId: 'flash-attention',
    points: [
      'GPU上のSRAMとHBM間のIO回数を最小化することで注意計算を高速化するアルゴリズム。',
      '数学的にはstandard attentionと同じ結果を返す（近似ではない）。',
      'メモリ使用量がO(n²)からO(n)に削減される利点がある。',
    ],
  },

  // ============================
  // 汎化性能 (dl-generalization)
  // ============================
  {
    termId: 'generalization',
    points: [
      '訓練誤差と汎化誤差（テスト誤差）の差が汎化ギャップ。これが小さいほど汎化性能が高い。',
      '過学習（訓練誤差は低いがテスト誤差が高い）と未学習（両方高い）を区別する。',
      'バイアス-バリアンスのトレードオフの概念と結びつけて理解する。',
    ],
  },
  {
    termId: 'regularization-theory',
    points: [
      '正則化はモデルの複雑さにペナルティを与え、汎化性能を向上させる理論的枠組み。',
      'オッカムの剃刀（よりシンプルなモデルを選好）の原理と関連付けて理解する。',
      '暗黙的正則化（SGDのノイズ、Early Stopping等）も正則化の一形態であることを押さえる。',
    ],
  },
  {
    termId: 'double-descent',
    points: [
      '従来のバイアス-バリアンストレードオフの「U字カーブ」を超えた現象。',
      'モデルのパラメータ数が訓練データ数を超える（補間閾値）付近でテスト誤差がピークになり、さらに増やすと再び減少する。',
      'エポック数に対しても同様の現象が観察される（Epoch-wise double descent）。',
    ],
  },
  {
    termId: 'lottery-ticket',
    points: [
      '密なネットワークには、独立に学習しても元のネットワークと同等の性能を達成できるスパースなサブネットワークが存在するという仮説。',
      'プルーニングとの関連が深い。学習後に枝刈りし、初期重みに戻して再学習する反復プルーニング手法で検証される。',
      '大規模モデルの効率化への示唆を与える重要な知見。',
    ],
  },
  {
    termId: 'neural-scaling-law',
    points: [
      'モデルサイズN、データ量D、計算量Cと性能の間にべき乗則 L ∝ N^(-α) の関係がある。',
      'Chinchilla Scaling Lawでは計算予算に対しモデルサイズとデータ量を均等にスケールすべきとされる。',
      'LLM開発において最適なリソース配分を決定するための重要な指針。',
    ],
  },
  {
    termId: 'flat-minima',
    points: [
      '損失曲面の平坦な最小値は鋭い最小値より汎化性能が高いとされる。',
      '大きいバッチサイズは鋭い最小値に収束しやすく、小さいバッチは平坦な最小値に収束しやすい傾向がある。',
      'SAMはこの仮説に基づき、明示的に平坦な最小値を探索する。',
    ],
  },
  {
    termId: 'sharpness-aware',
    points: [
      '損失値だけでなく損失曲面の鋭さ（sharpness）も同時に最小化する最適化手法。',
      '2ステップの最適化: 最も損失が増加する方向に摂動→その位置で勾配を計算し更新。',
      '計算コストが通常の最適化の約2倍だが、汎化性能の向上が見込める。',
    ],
    formula: 'min_w max_{||ε||≤ρ} L(w + ε)',
  },
  {
    termId: 'knowledge-distillation-gen',
    points: [
      '大きなTeacherモデルのsoftmax出力（soft targets）をStudentモデルに学習させる。',
      '温度パラメータTを上げてsoftmax出力を滑らかにし、クラス間の類似度情報を伝える。',
      'モデル圧縮だけでなく、Studentの汎化性能がTeacherを超えることもある。',
    ],
    formula: 'L = αL_CE(y, p_S) + (1-α)T²·L_KL(p_T/T, p_S/T)',
  },
];
