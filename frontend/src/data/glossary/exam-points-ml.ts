import type { TermExamPoints } from '@/types/glossary';

/**
 * 機械学習セクションの試験ポイント
 * E資格の出題傾向に基づいた学習ポイントを用語ごとにまとめる
 */
export const ML_EXAM_POINTS: TermExamPoints[] = [
  // ========================================
  // パターン認識 (ml-pattern)
  // ========================================
  {
    termId: 'knn',
    points: [
      'kの値が小さいと過学習、大きいと未学習になる。k=1のとき訓練誤差は0になることを押さえる。',
      '距離の定義（ユークリッド距離・マンハッタン距離）の違いが問われる。特徴量スケーリングが必須な理由も頻出。',
      '怠惰学習（lazy learning）の代表例。学習フェーズがなく、予測時に全データを走査するため計算コストが高い。',
    ],
  },
  {
    termId: 'k-means',
    points: [
      '初期重心の選び方で結果が変わる。k-means++による初期化手法が出題される。',
      '収束性は保証されるが、大域最適解への収束は保証されない点がよく問われる。',
      'エルボー法やシルエット係数によるクラスタ数kの決定方法を理解しておく。',
    ],
  },
  {
    termId: 'decision-tree',
    points: [
      '分割基準（情報ゲイン・ジニ不純度・エントロピー）の計算問題が頻出。',
      '過学習しやすい性質があり、剪定（pruning）やmax_depthによる制約が対策となる。',
      'CARTアルゴリズムは二分木を構築する。ID3やC4.5との違いも確認しておく。',
    ],
    formula: '情報ゲイン = H(親) - Σ(|子|/|親|) * H(子)、H = -Σ p_i log₂(p_i)',
  },
  {
    termId: 'random-forest',
    points: [
      'バギング＋特徴量のランダム選択で多様性を確保する。選択する特徴量数は√d（分類）やd/3（回帰）が目安。',
      'OOB（Out-of-Bag）スコアで交差検証なしに汎化性能を推定できる点が出題される。',
      '決定木との比較問題で「解釈性が低下する代わりに汎化性能が向上する」という関係を押さえる。',
    ],
  },
  {
    termId: 'svm',
    points: [
      'マージン最大化の定式化（双対問題）とKKT条件が出題される。サポートベクターのみが決定境界に影響する。',
      'ソフトマージンSVMのパラメータCの役割を理解する。C大→マージン小（過学習寄り）、C小→マージン大（未学習寄り）。',
      'ヒンジ損失 max(0, 1 - y*f(x)) の計算問題が出る。',
    ],
    formula: 'max α Σαᵢ - 1/2 ΣΣ αᵢαⱼyᵢyⱼK(xᵢ,xⱼ)  s.t. αᵢ≥0, Σαᵢyᵢ=0',
  },
  {
    termId: 'kernel-trick',
    points: [
      '高次元への明示的な写像φ(x)を計算せずにK(x,x\')=φ(x)・φ(x\')で内積を求められる点が重要。',
      '代表的なカーネル（線形・多項式・RBF/ガウシアン）の特徴と使い分けが問われる。',
      'RBFカーネルのパラメータγの影響を理解する。γ大→複雑な境界（過学習）、γ小→単純な境界。',
    ],
    formula: 'RBFカーネル: K(x,x\') = exp(-γ||x-x\'||²)',
  },
  {
    termId: 'naive-bayes',
    points: [
      '「特徴量間の条件付き独立性の仮定」が名前の由来。この仮定が成立しなくても実用上よく機能する。',
      'MAP推定とベイズの定理を用いた分類の導出過程を計算できるようにしておく。',
      'ゼロ頻度問題とラプラススムージング（加算スムージング）が出題される。',
    ],
    formula: 'P(C|x) ∝ P(C) Π P(xᵢ|C)',
  },
  {
    termId: 'logistic-regression',
    points: [
      'シグモイド関数σ(z) = 1/(1+e^(-z))の形状と出力の確率的解釈を理解する。',
      '損失関数は交差エントロピー（対数尤度の負値）。最小二乗法ではない点に注意。',
      '多クラス分類への拡張はソフトマックス回帰（多項ロジスティック回帰）。二値分類との違いを押さえる。',
    ],
    formula: 'P(y=1|x) = σ(wᵀx + b) = 1 / (1 + exp(-(wᵀx + b)))',
  },
  {
    termId: 'linear-regression',
    points: [
      '最小二乗法の正規方程式 w = (XᵀX)⁻¹Xᵀy の導出が頻出。XᵀXが正則でない場合の対処も問われる。',
      'L1正則化（Lasso）はスパースな解、L2正則化（Ridge）は重みの縮小という違いが重要。',
      '残差分析（残差プロット・正規性確認）と多重共線性の問題を理解しておく。',
    ],
    formula: 'w = (XᵀX)⁻¹Xᵀy、損失: L = 1/n Σ(yᵢ - wᵀxᵢ)²',
  },
  {
    termId: 'gmm',
    points: [
      'EMアルゴリズム（E-step: 負担率の計算、M-step: パラメータ更新）の流れを説明できるようにする。',
      'k-meansはGMMの特殊ケース（共分散行列が単位行列の定数倍、混合係数が等しい場合）と理解する。',
      'BIC/AICによるモデル選択（混合数の決定）が出題される。',
    ],
    formula: 'p(x) = Σₖ πₖ N(x|μₖ, Σₖ)  (πₖ: 混合係数)',
  },

  // ========================================
  // 機械学習の分類 (ml-class)
  // ========================================
  {
    termId: 'supervised',
    points: [
      '入力xと正解ラベルyのペア(x,y)から関数f:x→yを学習する。分類と回帰の違いを明確に区別する。',
      '教師なし・半教師あり・自己教師あり・強化学習との対比が頻出。それぞれの学習データの形式を把握する。',
      '代表的アルゴリズム（SVM、ロジスティック回帰、ニューラルネットワーク等）を挙げられるようにする。',
    ],
  },
  {
    termId: 'unsupervised',
    points: [
      'ラベルなしデータのみから構造を発見する。クラスタリング・次元削減・密度推定が主なタスク。',
      '評価が難しい点が教師あり学習との大きな違い。シルエット係数やエルボー法など間接的評価法を知る。',
      'オートエンコーダやGANも教師なし学習の一種として分類される場合がある。',
    ],
  },
  {
    termId: 'semi-supervised',
    points: [
      '少量のラベル付きデータと大量のラベルなしデータを組み合わせる。ラベル付けコストが高い実務で重要。',
      '代表手法（自己学習、共訓練、ラベル伝播法）の概要を押さえる。',
      '仮定（平滑性仮定、クラスタ仮定、多様体仮定）がどのように活用されるか理解する。',
    ],
  },
  {
    termId: 'self-supervised',
    points: [
      'データ自体からプリテキストタスクを作成して学習する。BERTのMLMやSimCLRの対比学習が代表例。',
      '教師なし学習との違い：自己教師ありは明示的に疑似ラベルを設計する点が異なる。',
      '大規模事前学習→ファインチューニングの流れ（転移学習）との関連を理解する。',
    ],
  },
  {
    termId: 'reinforcement',
    points: [
      '状態s、行動a、報酬r、方策πの基本要素を押さえる。マルコフ決定過程(MDP)が理論的基盤。',
      '価値ベース（Q学習、DQN）と方策ベース（REINFORCE、PPO）の違いが問われる。',
      '探索と利用のトレードオフ（ε-greedy法等）が頻出テーマ。',
    ],
  },
  {
    termId: 'classification',
    points: [
      '二値分類と多クラス分類を区別する。多クラスの手法（OvR、OvO、ソフトマックス）を把握する。',
      '出力が離散的なカテゴリである点が回帰タスクとの決定的な違い。',
      '評価指標として精度・適合率・再現率・F1スコア・AUCを使い分けられるようにする。',
    ],
  },
  {
    termId: 'regression-task',
    points: [
      '出力が連続値である点が分類との違い。損失関数にMSEやMAEを使用する。',
      '線形回帰・多項式回帰・Ridge回帰・Lasso回帰の使い分けが出題される。',
      '評価指標（MSE、RMSE、MAE、決定係数R²）の特性と使い分けを理解する。',
    ],
  },
  {
    termId: 'clustering',
    points: [
      'k-means、階層的クラスタリング、DBSCANの特徴と使い分けが頻出。',
      'DBSCANはクラスタ数を事前指定不要で、任意形状のクラスタを検出できる点が強み。',
      '評価指標（シルエット係数、Calinski-Harabasz指数）の計算と解釈を押さえる。',
    ],
  },
  {
    termId: 'dimensionality-reduction',
    points: [
      'PCA（主成分分析）は分散最大化、t-SNEは局所構造保持という目的の違いを理解する。',
      'PCAの固有値分解・特異値分解との関係が計算問題で出題される。寄与率の算出も重要。',
      '次元の呪いとの関連で、なぜ次元削減が必要かを説明できるようにする。',
    ],
  },
  {
    termId: 'feature-engineering',
    points: [
      '特徴量選択（フィルタ法・ラッパー法・埋め込み法）の違いと具体例を押さえる。',
      'カテゴリ変数のエンコーディング（ワンホット、ラベル、ターゲットエンコーディング）が問われる。',
      '特徴量の重要度（ランダムフォレストのfeature importance、SHAP値）を理解する。',
    ],
  },
  {
    termId: 'ensemble',
    points: [
      'バギング（並列・分散削減）とブースティング（逐次・バイアス削減）の違いが超頻出。',
      'スタッキング（メタ学習器で複数モデルを統合）の仕組みも出題される。',
      '弱学習器を組み合わせて強学習器を作るという基本原理を説明できるようにする。',
    ],
  },
  {
    termId: 'boosting',
    points: [
      'AdaBoost→GradientBoosting→XGBoost→LightGBMの発展の流れを理解する。',
      'AdaBoostは誤分類サンプルの重みを増やして逐次学習する。重み更新の計算が問われる。',
      '勾配ブースティングは残差（勾配）に対して新しい木をフィッティングする手法。',
    ],
    formula: 'AdaBoost重み更新: αₜ = 1/2 ln((1-εₜ)/εₜ)',
  },

  // ========================================
  // 機械学習の課題 (ml-issues)
  // ========================================
  {
    termId: 'overfitting',
    points: [
      '訓練誤差は小さいがテスト誤差が大きい状態。学習曲線の図から判定する問題が出る。',
      '対策（正則化、ドロップアウト、早期終了、データ拡張、交差検証）を網羅的に挙げられるようにする。',
      'モデルの複雑さ（パラメータ数）とデータ量の関係で発生する点を押さえる。',
    ],
  },
  {
    termId: 'underfitting',
    points: [
      '訓練誤差もテスト誤差も大きい状態。モデルが単純すぎるか、学習が不十分な場合に発生。',
      '対策はモデルの複雑さを増す・特徴量を追加する・学習時間を延ばすこと。',
      '過学習とセットで出題されることが多い。学習曲線の形状から区別できるようにする。',
    ],
  },
  {
    termId: 'bias-variance',
    points: [
      '汎化誤差 = バイアス² + バリアンス + ノイズ の分解式が超重要。',
      'バイアスが高い→未学習（モデルが単純すぎる）。バリアンスが高い→過学習（モデルが複雑すぎる）。',
      'アンサンブル手法との関連：バギングはバリアンス削減、ブースティングはバイアス削減に効果的。',
    ],
    formula: 'E[(y - f̂(x))²] = Bias(f̂)² + Var(f̂) + σ²',
  },
  {
    termId: 'curse-of-dim',
    points: [
      '次元が増えるとデータが疎になり、距離ベースの手法（kNN等）の性能が低下する。',
      '必要なサンプル数は次元の指数関数的に増加するという直感を持つ。',
      '対策は次元削減（PCA等）や特徴量選択。L1正則化によるスパース化も有効。',
    ],
  },
  {
    termId: 'class-imbalance',
    points: [
      'リサンプリング（オーバーサンプリング：SMOTE、アンダーサンプリング）の手法と特徴を理解する。',
      'コスト考慮型学習（クラスの重み付け）やFocal Lossによる対策が出題される。',
      '不均衡時に正解率(accuracy)が不適切な指標となる理由を説明できるようにする。',
    ],
  },
  {
    termId: 'data-augmentation',
    points: [
      '画像分野（回転、反転、クロップ、色変換、Mixup、CutMix）の具体例が頻出。',
      'テキスト分野（同義語置換、バックトランスレーション）や音声分野の手法も押さえる。',
      '過学習防止とデータ不足対策として使用される。学習時のみ適用しテスト時には適用しない点に注意。',
    ],
  },
  {
    termId: 'missing-data',
    points: [
      '欠損パターン（MCAR、MAR、MNAR）の違いと適切な対処法の対応関係が出題される。',
      '単純な手法（削除、平均値補完）と高度な手法（多重代入法、kNN補完）を使い分ける。',
      '欠損メカニズムを無視した処理がバイアスを生む可能性がある点を理解する。',
    ],
  },
  {
    termId: 'outlier',
    points: [
      '検出手法（IQR法、Zスコア、Isolation Forest、Local Outlier Factor）の特徴を押さえる。',
      '外れ値の処理方針（除去・変換・そのまま保持）はタスクに依存する。一律除去は危険。',
      'MSEは外れ値に敏感、MAEは頑健という損失関数の選択との関連を理解する。',
    ],
  },
  {
    termId: 'feature-scaling',
    points: [
      '標準化（平均0、分散1）と最小最大スケーリング（0~1に変換）の使い分けが頻出。',
      '距離ベースの手法（kNN、SVM、k-means）や勾配降下法で必須。決定木系は不要。',
      'テストデータには訓練データの統計量を使ってスケーリングする点を間違えやすい。',
    ],
    formula: '標準化: z = (x - μ) / σ、MinMax: x\' = (x - min) / (max - min)',
  },
  {
    termId: 'label-noise',
    points: [
      'ラベルノイズ率が高いとモデルがノイズを学習し、汎化性能が低下する。',
      '対策（クリーニング、ノイズ耐性のある損失関数、正則化の強化）を理解する。',
      'クラウドソーシングでのアノテーションにおけるノイズ軽減（多数決、品質管理）も出題範囲。',
    ],
  },
  {
    termId: 'covariate-shift',
    points: [
      '訓練データP_train(x)とテストデータP_test(x)の入力分布が異なる状況。P(y|x)は変わらないと仮定。',
      '重要度重み付き学習（importance weighting）による対処法が出題される。',
      'バッチ正規化が内部共変量シフトへの対策として提案された背景を押さえる。',
    ],
  },
  {
    termId: 'domain-adaptation',
    points: [
      'ソースドメイン（ラベルあり）→ターゲットドメイン（ラベルなし/少量）への知識転移が目的。',
      '敵対的ドメイン適応（DANN）やドメイン不変な特徴表現の学習が出題される。',
      '転移学習との関係と違いを明確にする。ドメイン適応は転移学習の一形態。',
    ],
  },

  // ========================================
  // 検証集合 (ml-validation)
  // ========================================
  {
    termId: 'train-test-split',
    points: [
      '一般的な分割比率（訓練:テスト = 7:3や8:2）を把握する。検証用データも含めて3分割する場合もある。',
      '時系列データではランダム分割ではなく時間順に分割する必要がある点が出題される。',
      'データリーク（テストデータの情報が訓練に混入）を防ぐため、前処理は分割後に行う。',
    ],
  },
  {
    termId: 'cross-validation',
    points: [
      'ホールドアウト法と比較して全データを訓練と検証に使用できるため、評価の分散が小さい。',
      'Leave-One-Out（LOO）はデータ数=k。計算コストが高いが小規模データで有用。',
      'モデル選択（ハイパーパラメータ調整）に使い、最終評価は別途テストデータで行う点を押さえる。',
    ],
  },
  {
    termId: 'k-fold',
    points: [
      'k=5またはk=10が一般的。kが大きいと計算コスト増だがバイアス減、kが小さいとその逆。',
      '各foldのスコアの平均と標準偏差を報告することで、モデルの安定性も評価できる。',
      '層化k分割（Stratified k-Fold）との違いを理解し、分類タスクでは層化を使うべき理由を説明できるようにする。',
    ],
  },
  {
    termId: 'holdout',
    points: [
      '最もシンプルだがデータ分割のランダム性に結果が依存する。データ量が少ない場合は不安定。',
      '交差検証と比較した長所（計算コストが低い）と短所（評価の分散が大きい）を理解する。',
      '大規模データセットでは交差検証の計算コストが高いため、ホールドアウト法が実用的。',
    ],
  },
  {
    termId: 'stratified',
    points: [
      'クラス比率を各分割で維持する。不均衡データでは層化しないと偏った分割になる。',
      '分類タスクの交差検証では層化抽出がデフォルトであるべき理由を説明できるようにする。',
      '回帰タスクでも目的変数の分布に基づいた層化が有効な場合がある。',
    ],
  },
  {
    termId: 'hyperparameter',
    points: [
      'パラメータ（学習で決まる）とハイパーパラメータ（人が設定する）の区別が基本。',
      '代表例を挙げられるようにする：学習率、バッチサイズ、正則化係数、層数、ユニット数など。',
      '検証データを用いて選択する。テストデータで選んではいけない点がよく問われる。',
    ],
  },
  {
    termId: 'grid-search',
    points: [
      '候補値の全組み合わせを試すため、パラメータ数が多いと計算量が指数的に増大する。',
      'ランダムサーチと比較して「低次元のとき確実」「高次元のとき非効率」という特性を理解する。',
      '交差検証と組み合わせて使用する（GridSearchCV）のが一般的。',
    ],
  },
  {
    termId: 'random-search',
    points: [
      '高次元パラメータ空間ではグリッドサーチより効率的（Bergstra & Bengio 2012）。',
      '重要でないパラメータ軸にも均等に探索点を配置するグリッドサーチの無駄を避けられる。',
      '探索回数（budget）を指定でき、計算資源に応じた柔軟な探索が可能。',
    ],
  },
  {
    termId: 'early-stopping',
    points: [
      '検証損失がpatience回連続で改善しなければ学習を打ち切る。暗黙的な正則化効果がある。',
      '学習率やエポック数を厳密に決めなくても過学習を防げる実用的な手法。',
      'モデルの保存タイミング（ベストモデルの保存）と組み合わせることが重要。',
    ],
  },
  {
    termId: 'bayesian-optim',
    points: [
      'ガウス過程で目的関数のサロゲートモデルを構築し、獲得関数（EI, UCB等）で次の評価点を決定する。',
      '評価コストが高い（1回の学習に時間がかかる）場合にグリッドサーチやランダムサーチより効率的。',
      '逐次的な探索のため並列化が難しいという制約がある。',
    ],
  },

  // ========================================
  // 性能指標 (ml-metrics)
  // ========================================
  {
    termId: 'accuracy',
    points: [
      'クラス不均衡時に多数派クラスを常に予測するだけで高い正解率が出てしまう問題を理解する。',
      '混同行列からの計算 (TP+TN)/(TP+TN+FP+FN) を確実にできるようにする。',
      '不均衡データではF1スコアやAUCの方が適切な指標であることを説明できるようにする。',
    ],
    formula: 'Accuracy = (TP + TN) / (TP + TN + FP + FN)',
  },
  {
    termId: 'precision',
    points: [
      '偽陽性（FP）を減らしたい場面で重視する。スパムフィルタで正常メールを誤検出したくない場合など。',
      '再現率とのトレードオフ関係を理解する。閾値を上げると適合率↑・再現率↓。',
      '混同行列の「予測が陽性の列」に着目する。計算ミスしやすいので分母を確認する習慣をつける。',
    ],
    formula: 'Precision = TP / (TP + FP)',
  },
  {
    termId: 'recall',
    points: [
      '偽陰性（FN）を減らしたい場面で重視する。病気の見落としを防ぎたい医療診断など。',
      '感度（Sensitivity）、真陽性率（TPR）と同義であることを押さえる。',
      '混同行列の「実際が陽性の行」に着目する。適合率との分母の違いを間違えない。',
    ],
    formula: 'Recall = TP / (TP + FN)',
  },
  {
    termId: 'f1-score',
    points: [
      '適合率と再現率の調和平均。どちらか一方が極端に低いと値が下がる性質が重要。',
      'マクロF1（クラスごとに計算して平均）とマイクロF1（全体で計算）の違いが出題される。',
      'Fβスコアでβ>1なら再現率重視、β<1なら適合率重視となる点も押さえる。',
    ],
    formula: 'F1 = 2 * Precision * Recall / (Precision + Recall)',
  },
  {
    termId: 'confusion-matrix',
    points: [
      'TP（真陽性）、FP（偽陽性）、FN（偽陰性）、TN（真陰性）の4要素の位置を確実に覚える。',
      '混同行列からAccuracy、Precision、Recall、F1を全て導出できるようにする。',
      '多クラス分類の混同行列では対角線上が正解、非対角要素が誤分類を表す。',
    ],
  },
  {
    termId: 'roc-curve',
    points: [
      '横軸がFPR（偽陽性率）、縦軸がTPR（真陽性率＝再現率）。閾値を変化させてプロットする。',
      'ランダム分類器は対角線上（AUC=0.5）。それより上なら意味のある分類ができている。',
      'PR曲線（Precision-Recall曲線）との使い分けを理解する。不均衡データではPR曲線が有用。',
    ],
    formula: 'TPR = TP/(TP+FN)、FPR = FP/(FP+TN)',
  },
  {
    termId: 'auc',
    points: [
      'AUC=1.0で完全分類、AUC=0.5でランダム。閾値に依存しない総合的な性能指標。',
      'ランダムに選んだ陽性サンプルが陰性サンプルより高いスコアを持つ確率という解釈を知る。',
      'クラス不均衡でも比較的安定した指標だが、FPの絶対数が重要な場合はPR-AUCを使う。',
    ],
  },
  {
    termId: 'log-loss',
    points: [
      '確率出力の質を評価する指標。予測確率が正解ラベルから離れるほどペナルティが急増する。',
      '交差エントロピー損失と本質的に同じ。ロジスティック回帰やニューラルネットの損失関数として使用。',
      '0に近い確率を正解クラスに割り当てると-log(p)が非常に大きくなり損失が爆発する点に注意。',
    ],
    formula: 'LogLoss = -1/n Σ [yᵢ log(pᵢ) + (1-yᵢ) log(1-pᵢ)]',
  },
  {
    termId: 'mse',
    points: [
      '外れ値に敏感（二乗するため大きな誤差が強調される）。MAEとの使い分けが出題される。',
      '微分可能で最適化しやすいため、損失関数として広く使用される。',
      'RMSEは√MSEで元の単位に戻せるため解釈しやすい。MSEとの関係を押さえる。',
    ],
    formula: 'MSE = 1/n Σ(yᵢ - ŷᵢ)²',
  },
  {
    termId: 'mae',
    points: [
      '外れ値に頑健（絶対値のため大きな誤差が過度に強調されない）。中央値に対応する。',
      '0点で微分不可能なため、最適化にはサブ勾配法を使う必要がある。',
      'MSEとの比較問題が頻出。「外れ値が多いデータにはMAE」と覚える。',
    ],
    formula: 'MAE = 1/n Σ|yᵢ - ŷᵢ|',
  },
  {
    termId: 'r-squared',
    points: [
      'R²=1で完全な予測、R²=0で平均値予測と同じ。負の値になることもある（平均値以下の予測）。',
      '説明変数を増やすとR²は必ず増加する。自由度調整済みR²との違いを理解する。',
      '回帰モデルの「説明力」を表す指標であり、因果関係を示すものではない点に注意。',
    ],
    formula: 'R² = 1 - Σ(yᵢ - ŷᵢ)² / Σ(yᵢ - ȳ)²',
  },
];
