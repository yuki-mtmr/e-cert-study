import type { MemorizationQuestion, QuizMeta } from '@/types/memorization';

/** クイズメタデータ */
export const QUIZ_META: QuizMeta = {
  title: 'E資格過去問クイズ',
  version: '1.0',
  totalQuestions: 50,
  categories: [
    '最適化',
    '逆伝播・連鎖律',
    '確率・ベイズ',
    '情報理論',
    '距離・類似度',
    '機械学習の基礎',
    'ニューラルネットワーク基礎',
    '正則化・初期化',
    'CNN',
    'RNN・LSTM・GRU',
    'Transformer',
    '物体検出',
    '生成モデル',
    '強化学習',
    '説明可能AI',
  ],
};

/** 暗記クイズ全50問 */
export const MEMORIZATION_QUESTIONS: MemorizationQuestion[] = [
  {
    id: 1,
    category: '最適化',
    question: 'Momentumの更新式 v_t = (あ) に入る最も適切な選択肢を選べ。\n\nv_t = (あ)\nθ = θ − v_t',
    choices: [
      'γv_{t-1} − η∇_θ J(θ)',
      'γv_{t-1} + η∇_θ J(θ)',
      'ηv_{t-1} + γ∇_θ J(θ)',
      'ηv_{t-1} − γ∇_θ J(θ)',
    ],
    answer: 'B',
    hint: 'v_{t-1}にはMomentum係数γ、勾配には学習率η。θ=θ−v_tで引いてるから勾配は+。',
  },
  {
    id: 2,
    category: '最適化',
    question: 'NAG（Nesterov Accelerated Gradient）の更新式 v_t = (い) に入る最も適切な選択肢を選べ。\n\nv_t = (い)\nθ = θ − v_t',
    choices: [
      'γv_{t-1} − η∇_θ J(θ − v_{t-1})',
      '(1−γ)v_{t-1} + ηγ∇_θ J(θ)',
      'γv_{t-1} + (1−γ)∇_θ J(θ)',
      'γv_{t-1} + η∇_θ J(θ − γv_{t-1})',
    ],
    answer: 'D',
    hint: 'NAGの核心は「先読み」。勾配の引数がθではなくθ−γv_{t-1}に変わるだけ。',
  },
  {
    id: 3,
    category: '最適化',
    question: 'Pathological Curvatureについて、以下のうち最も不適切な選択肢を一つ選べ。',
    choices: [
      'Pathological Curvatureを解決する方法の一つに、大きなミニバッチサイズを利用する方法がある。',
      'Pathological Curvatureは、振動を起こしてしまうために、局所的な最小値に陥りやすくなる。',
      'Pathological Curvatureでは、目的関数の減少量が非常に小さくなり、曲率が非常に大きくなることで学習が滞ってしまう。',
      'Momentumは、過去の勾配の変化を利用して振動を抑えることを目的としており、Pathological Curvatureの問題を緩和する。',
    ],
    answer: 'B',
    hint: 'Pathological Curvatureは谷が細長くて学習が遅くなる問題。局所最小値の問題とは別。',
  },
  {
    id: 4,
    category: '逆伝播・連鎖律',
    question: '合成関数g(f(x))のxに対する微分を表す式として最も適切な選択肢を選べ。',
    choices: [
      '(∂f/∂g) × (∂f/∂x)',
      '(∂f/∂g) × (∂g/∂x)',
      '(∂g/∂f) × (∂f/∂x)',
      '(∂g/∂f) × (∂g/∂x)',
    ],
    answer: 'C',
    hint: '連鎖律：外側から微分 × 内側の微分 = (∂g/∂f) × (∂f/∂x)',
  },
  {
    id: 5,
    category: '逆伝播・連鎖律',
    question: '3層全結合型NNで L=(t−y)², y=w₂σ(w₁x+b₁)+b₂ のとき、∂L/∂w₂=(あ)、∂L/∂w₁=(い) の組み合わせとして最も適切な選択肢を選べ。',
    choices: [
      '(あ) −2(t−y)h  (い) −2(t−y)w₂h(1−h)x',
      '(あ) −2(t−y)h  (い) −2(t−y)w₁h(1−h)x',
      '(あ) −2(t+y)h  (い) −2(t+y)w₂h(1+h)x',
      '(あ) −2(t+y)h  (い) −2(t+y)w₁h(1+h)x',
    ],
    answer: 'A',
    hint: 'L=(t−y)²だから微分は−2(t−y)。w₁の勾配は連鎖律で奥に進む：×w₂×σ\'(=h(1−h))×x',
  },
  {
    id: 6,
    category: '確率・ベイズ',
    question: '事象Aと事象Bにおけるベイズ則を示す式として最も適切な選択肢を選べ。',
    choices: [
      'P(A|B) = P(A|B)P(B) / P(A)',
      'P(A|B) = P(B|A)P(A) / P(B)',
      'P(B|A) = P(B|A)P(B) / P(A)',
      'P(B|A) = P(A|B)P(A) / P(B)',
    ],
    answer: 'C',
    hint: 'ベイズ則：P(B|A) = P(A|B)P(B)/P(A)。左辺と分子・分母の対応を確認。',
  },
  {
    id: 7,
    category: '確率・ベイズ',
    question: 'MAP推定の式として最も適切な選択肢を選べ。',
    choices: [
      'θ_MAP = arg max_θ p(x|θ)',
      'θ_MAP = arg max_θ p(θ|x)',
      'θ_MAP = arg max_θ log p(x|θ) + log p(x)',
      'θ_MAP = arg max_θ log p(θ|x) + log p(θ)',
    ],
    answer: 'B',
    hint: 'MAP = 事後確率p(θ|x)を最大化。Aはp(x|θ)で尤度のみ=最尤推定。',
  },
  {
    id: 8,
    category: '情報理論',
    question: 'エントロピーH(X)=(あ)、交差エントロピーH=(い) の組み合わせとして最も適切な選択肢を選べ。',
    choices: [
      '(あ) ΣP(x)log₂P(x)  (い) −ΣP(x)log₂Q(x)',
      '(あ) −ΣP(x)log₂P(x)  (い) −ΣP(x)log₂Q(x)',
      '(あ) −ΣP(x)log₂P(x)  (い) −Σlog₂P(x)log₂Q(x)',
      '(あ) ΣP(x)log₂P(x)  (い) −Σlog₂P(x)log₂Q(x)',
    ],
    answer: 'B',
    hint: 'エントロピー：−ΣP(x)logP(x)。交差エントロピー：−ΣP(x)logQ(x)。Qに変わるだけ。',
  },
  {
    id: 9,
    category: '情報理論',
    question: '自己情報量I(A)=(あ)、相互情報量I(X;Y)=(い) の組み合わせとして最も適切な選択肢を選べ。',
    choices: [
      '(あ) log₂P(A)  (い) ΣΣP(x,y)log₂(P(x,y)/P(x)P(y))',
      '(あ) log₂P(A)  (い) −ΣΣP(x,y)log₂(P(x,y)/P(x)P(y))',
      '(あ) −log₂P(A)  (い) −ΣΣP(x,y)log₂(P(x,y)/P(x)P(y))',
      '(あ) −log₂P(A)  (い) ΣΣP(x,y)log₂(P(x,y)/P(x)P(y))',
    ],
    answer: 'D',
    hint: '自己情報量は−log₂P(A)。相互情報量は常に非負で符号は+。',
  },
  {
    id: 10,
    category: '情報理論',
    question: '条件付きエントロピーH(X|Y)の式として最も適切な選択肢を選べ。',
    choices: [
      'ΣΣP(x,y)log₂P(x|y)',
      'ΣΣP(x,y)log₂P(y|x)',
      '−ΣΣP(x,y)log₂P(x|y)',
      '−ΣΣP(x,y)log₂P(y|x)',
    ],
    answer: 'C',
    hint: 'H(X|Y)のlogの中はP(x|y)。エントロピーだから−符号。',
  },
  {
    id: 11,
    category: '情報理論',
    question: '結合エントロピーH(X,Y) = (あ) を満たす式として正しいものを選べ。',
    choices: [
      'H(X) + H(Y) + I(X;Y)',
      'H(X) + H(Y) − I(X;Y)',
      'H(X) − H(Y|X)',
      'H(X) + H(Y) − H(X|Y) − H(Y|X)',
    ],
    answer: 'B',
    hint: 'ベン図と同じ：個別の合計から重複（相互情報量）を引く。',
  },
  {
    id: 12,
    category: '情報理論',
    question: 'KLダイバージェンスについて、以下のうち最も適切な選択肢を選べ。',
    choices: [
      'D_KL(p||q) = ∫P(x)log(Q(x)/P(x))dx',
      '同じ確率分布間のKLダイバージェンスの値は0になる。',
      '相互情報量I(X;Y)は、I(X;Y)=D_KL(P(x,y)||P(x)P(y))と表すことができる。',
      'KLダイバージェンスを平滑化した指標は相対エントロピーと呼ばれている。',
    ],
    answer: 'A',
    hint: 'Aはlog内が逆（Q/P）。正しくはlog(P/Q)。B,C,Dは全て正しい。不適切を選ぶならA。※問題文が「最も適切」なら注意。',
  },
  {
    id: 13,
    category: '情報理論',
    question: 'JSダイバージェンスの式として最も適切な選択肢を選べ。m = ½P(x) + ½Q(x) とする。',
    choices: [
      'D_JS(p||q) = D_KL(m||p) + D_KL(m||q)',
      'D_JS(p||q) = D_KL(p||m) + D_KL(q||m)',
      'D_JS(p||q) = ½D_KL(m||p) + ½D_KL(m||q)',
      'D_JS(p||q) = ½D_KL(p||m) + ½D_KL(q||m)',
    ],
    answer: 'D',
    hint: 'JS = ½ × 各分布から中間分布mへのKL。½が付く + 引数はp||mとq||m。',
  },
  {
    id: 14,
    category: '距離・類似度',
    question: '点a,bのユークリッド距離(あ)、マンハッタン距離(い)、Lp距離(う)の組み合わせとして最も適切な選択肢を選べ。（差は(3,4)）',
    choices: [
      '(あ)5 (い)7 (う)(Σ|x_i|^{2p})^{1/p}',
      '(あ)5 (い)7 (う)(Σ|x_i|^p)^{1/p}',
      '(あ)7 (い)5 (う)(Σ|x_i|^{2p})^{1/p}',
      '(あ)7 (い)5 (う)(Σ|x_i|^p)^{1/p}',
    ],
    answer: 'B',
    hint: 'ユークリッド=√(9+16)=5、マンハッタン=3+4=7、Lp=(Σ|x_i|^p)^{1/p}。',
  },
  {
    id: 15,
    category: '距離・類似度',
    question: 'x=(0,4), y=(3,4)のコサイン類似度(あ)とマハラノビス距離(い)の組み合わせとして最も適切な選択肢を選べ。',
    choices: [
      '(あ)0.8 (い)√((x−y)ᵀΣ⁻¹(x−y))',
      '(あ)0.8 (い)√((x+y)ᵀΣ(x+y))',
      '(あ)0.6 (い)√((x−y)ᵀΣ⁻¹(x−y))',
      '(あ)0.6 (い)√((x+y)ᵀΣ(x+y))',
    ],
    answer: 'A',
    hint: 'コサイン類似度=(0·3+4·4)/(4·5)=16/20=0.8。マハラノビスは(x−y)とΣ⁻¹（逆行列）。',
  },
  {
    id: 16,
    category: '機械学習の基礎',
    question: 'バイアス²、バリアンス、ノイズの式の組み合わせとして最も適切な選択肢を選べ。',
    choices: [
      'バイアス²: ∫{E_D[y(x;D)]−h(x)}²dx  バリアンス: ∫E_D[{y(x;D)−E_D[y(x;D)]}²]dx  ノイズ: ∬{h(x)−t}²p(x,t)dxdt',
      'バイアス²: ∫E_D[y(x;D)−E_D[y(x;D)]]²dx  バリアンス: ∫{E_D[y(x;D)]−h(x)}²dx  ノイズ: ∬{h(x)−t}²p(x,t)dxdt',
      'バイアス²: ∬{h(x)−t}²p(x,t)dxdt  バリアンス: ∫E_D[{y−E_D[y]}²]dx  ノイズ: ∫{E_D[y]−h(x)}²dx',
      'バイアス²: ∫{E_D[y]−h(x)}²dx  バリアンス: ∬{h(x)−t}²p(x,t)dxdt  ノイズ: ∫E_D[{y−E_D[y]}²]dx',
    ],
    answer: 'A',
    hint: 'バイアス=期待値vs真の値、バリアンス=モデルのばらつき、ノイズ=真の値vsデータ。',
  },
  {
    id: 17,
    category: '機械学習の基礎',
    question: '(あ)MAE、(い)MSE、(う)RMSEの計算式の組み合わせとして最も適切な選択肢を選べ。',
    choices: [
      '(あ)1/N Σ(y_i−ŷ_i)² (い)1/N Σ|y_i−ŷ_i| (う)√(1/N Σ(y_i−ŷ_i)²)',
      '(あ)|y_i−ŷ_i| (い)1/N Σ(y_i−ŷ_i)² (う)√(1/N Σ(y_i−ŷ_i)²)',
      '(あ)1/N Σ(y_i−ŷ_i)² (い)1/N Σ|y_i−ŷ_i| (う)1/N Σ√((y_i−ŷ_i)²)',
      '(あ)1/N Σ|y_i−ŷ_i| (い)1/N Σ(y_i−ŷ_i)² (う)1/N Σ√((y_i−ŷ_i)²)',
    ],
    answer: 'A',
    hint: 'MAE=絶対値、MSE=二乗、RMSE=√MSE（全体にルート）。名前そのまま。',
  },
  {
    id: 18,
    category: 'ニューラルネットワーク基礎',
    question: '多層パーセプトロンで活性化関数(あ)と出力層の式(い)の組み合わせとして最も適切な選択肢を選べ。',
    choices: [
      '(あ)1/(1−e^{−x}) (い)f(w³_{1,1}a²₁+w³_{1,2}a²₂+b³₁)',
      '(あ)1/(1+e^{−x}) (い)f(w³_{1,1}a²₁+w³_{1,2}a²₂+b³₁)',
      '(あ)1/(1−e^{−x}) (い)f(w³_{1,1}a²₁×w³_{1,2}a²₂×b³₁)',
      '(あ)1/(1+e^{−x}) (い)f(w³_{1,1}a²₁×w³_{1,2}a²₂×b³₁)',
    ],
    answer: 'B',
    hint: 'シグモイド=1/(1+e^{−x})。出力は重み付き和+バイアス（加算、掛け算ではない）。',
  },
  {
    id: 19,
    category: '最適化',
    question: '確率的勾配降下法（SGD）のミニバッチ更新式として最も適切な選択肢を選べ。',
    choices: [
      'θ = θ − η·∇_θ J(θ; x_{i:i+n}, y_{i:i+n})',
      'θ = η·∇_θ J(θ; x_{i:i+n}, y_{i:i+n}) − θ',
      'θ = η·θ − ∇_θ J(θ; x_{i:i+n}, y_{i:i+n})',
      'θ = η(θ − ∇_θ J(θ; x_{i:i+n}, y_{i:i+n}))',
    ],
    answer: 'A',
    hint: 'SGDの基本形：θ = θ − η∇J。θから学習率×勾配を引く。',
  },
  {
    id: 20,
    category: '逆伝播・連鎖律',
    question: '全結合層の計算グラフ(y=X·W)で、Xへの勾配(あ)とWへの勾配(い)の組み合わせとして最も適切な選択肢を選べ。',
    choices: [
      '(あ)∂L/∂y · Wᵀ  (い)∂L/∂y · Xᵀ',
      '(あ)∂L/∂y · Wᵀ  (い)Xᵀ · ∂L/∂y',
      '(あ)Wᵀ · ∂L/∂y  (い)∂L/∂y · Xᵀ',
      '(あ)Wᵀ · ∂L/∂y  (い)Xᵀ · ∂L/∂y',
    ],
    answer: 'B',
    hint: '転置は相方の側に付く。X勾配=∂L/∂y·Wᵀ、W勾配=Xᵀ·∂L/∂y。',
  },
  {
    id: 21,
    category: '逆伝播・連鎖律',
    question: 'シグモイド層の計算グラフで、×ノード(あ)と+ノード(い)の逆伝播の組み合わせとして最も適切な選択肢を選べ。',
    choices: [
      '(あ)∂L/∂y · y²exp(−x)  (い)−∂L/∂y · y²',
      '(あ)−∂L/∂y · y²exp(−x)  (い)−∂L/∂y · y²',
      '(あ)∂L/∂y · y²exp(−x)  (い)∂L/∂y · y²',
      '(あ)−∂L/∂y · y²exp(−x)  (い)∂L/∂y · y²',
    ],
    answer: 'B',
    hint: '1/uの微分=−y²。expノードはe^{−x}を掛ける。+ノードはそのまま通す。',
  },
  {
    id: 22,
    category: '最適化',
    question: 'AdaGradのG_{t,i}=(あ)とRMSPropのG_{t,i}=(い)の組み合わせとして最も適切な選択肢を選べ。',
    choices: [
      '(あ)γG_{t-1,i}+(1−γ)g²_{t,i}  (い)G_{t-1,i}+g²_{t,i}',
      '(あ)G_{t-1,i}+g²_{t,i}  (い)γG_{t-1,i}+(1−γ)g²_{t,i}',
      '(あ)γG_{t-1,i}+(1−γ)g_{t,i}  (い)G_{t-1,i}+g_{t,i}',
      '(あ)G_{t-1,i}+g_{t,i}  (い)γG_{t-1,i}+(1−γ)g_{t,i}',
    ],
    answer: 'B',
    hint: 'AdaGrad=単純累積(γなし)、RMSProp=指数移動平均(γ付き)。どちらもg²。',
  },
  {
    id: 23,
    category: '最適化',
    question: 'Adamの更新式で m_t=(あ)、v_t=(い) の組み合わせとして最も適切な選択肢を選べ。',
    choices: [
      '(あ)β₁m_{t-1}+(1−β₁)g²_t  (い)β₂v_{t-1}+(1−β₂)g_t',
      '(あ)β₁m_{t-1}−(1−β₁)g_t  (い)β₂v_{t-1}−(1−β₂)g²_t',
      '(あ)β₁m_{t-1}+(1−β₁)g²_t  (い)β₂v_{t-1}+(1−β₂)g²_t',
      '(あ)β₁m_{t-1}+(1−β₁)g_t  (い)β₂v_{t-1}+(1−β₂)g²_t',
    ],
    answer: 'D',
    hint: 'm_t=1次モーメント(勾配の移動平均、二乗なし)、v_t=2次モーメント(勾配二乗の移動平均)。符号はどちらも+。',
  },
  {
    id: 24,
    category: '正則化・初期化',
    question: 'Xavier法とHe法のパラメータ初期化について最も適切な選択肢を選べ。',
    choices: [
      'Xavier法: U(−√(6/(n_in+n_out)), √(6/(n_in+n_out)))、sigmoid/tanh向け',
      'He法: U(−√(6/(n_in+n_out)), √(6/(n_in+n_out)))、ReLU向け',
      'Xavier法: U(−√(6/n_in), √(6/n_in))、sigmoid/tanh向け',
      'He法: U(−√(6/n_in), √(6/n_in))、softmax/恒等関数向け',
    ],
    answer: 'A',
    hint: 'Xavier=両方(in+out)=sigmoid/tanh、He=入力だけ(in)=ReLU。',
  },
  {
    id: 25,
    category: '正則化・初期化',
    question: 'ドロップアウトとドロップコネクトについて、図(あ)と式(う)の組み合わせとして最も適切な選択肢を選べ。',
    choices: [
      '(あ)ドロップコネクト (う)y=(r⊙W)x',
      '(あ)ドロップアウト (う)y=(r⊙W)x',
      '(あ)ドロップコネクト (う)y=r⊙(Wx)',
      '(あ)ドロップアウト (う)y=r⊙(Wx)',
    ],
    answer: 'D',
    hint: 'ドロップアウト=ノード消す=出力にマスク r⊙(Wx)。ドロップコネクト=重みマスク (r⊙W)x。',
  },
  {
    id: 26,
    category: 'CNN',
    question: '入力7×7、フィルタ3×3、ストライド1の畳み込み後の出力サイズ(あ)と一般化した出力サイズの式(い)として最も適切な選択肢を選べ。',
    choices: [
      '(あ)4×4 (い)⌊(W+2P−FW)/S⌋×⌊(H+2P−FH)/S⌋',
      '(あ)4×4 (い)(⌊(W+2P−FW)/S⌋+1)×(⌊(H+2P−FH)/S⌋+1)',
      '(あ)5×5 (い)(⌊(W+2P−FW)/S⌋+1)×(⌊(H+2P−FH)/S⌋+1)',
      '(あ)5×5 (い)⌊(W+2P−FW)/S⌋×⌊(H+2P−FH)/S⌋',
    ],
    answer: 'C',
    hint: '(7−3)/1+1=5。一般式は⌊(W+2P−FW)/S⌋+1。',
  },
  {
    id: 27,
    category: 'RNN・LSTM・GRU',
    question: 'RNNの計算グラフの一部を表した図として最も適切な選択肢を選べ。',
    choices: [
      '隠れ層間の接続が点線で弱い図',
      '隠れ層間に双方向の接続がある図',
      '各時刻でx_t→h_t→y_t、隠れ層が一方向に接続する図',
      '系列構造でなく1本のチェーンの図',
    ],
    answer: 'C',
    hint: 'RNNは各時刻で入力→隠れ層→出力、隠れ層は前の時刻から一方向に接続。',
  },
  {
    id: 28,
    category: 'RNN・LSTM・GRU',
    question: 'RNNの順伝播で状態hの入力 z^h_t = (あ) に当てはまる式として最も適切な選択肢を選べ。',
    choices: [
      'W^{hx}x_t + W^{hh}h_{t-1}',
      'W^{hx}x_{t-1} + W^{hh}h_{t-1}',
      'W^{yh}h_t',
      'W^{hx}x_t + W^{hh}h_{t-1} + W^{yh}h_t',
    ],
    answer: 'A',
    hint: '隠れ状態=現在の入力x_tと前の隠れ状態h_{t-1}の線形結合。',
  },
  {
    id: 29,
    category: 'RNN・LSTM・GRU',
    question: 'BPTT法で dL/dW^{yh}=(あ)、dL/dW^{hh}=(い) の組み合わせとして最も適切な選択肢を選べ。',
    choices: [
      '(あ)dL/dh_t (dh_t/dW^{yh})ᵀ  (い)dL/dz^h_t · h_t^T',
      '(あ)dL/dz^y_t (dz^y_t/dW^{yh})ᵀ  (い)dL/dz^h_t · h_{t-1}^T',
      '(あ)dL/dy_t (dy_t/dW^{yh})ᵀ  (い)dL/dz^h_t · x_t^T',
      '(あ)dL/dz^h_t (dz^h_t/dW^{yh})ᵀ  (い)dL/dz^h_t · x_{t-1}^T',
    ],
    answer: 'C',
    hint: 'W^{yh}勾配: z^y=W^{yh}h_tだからdL/dz^y_t·h_tᵀ。W^{hh}勾配: z^h=W^{hh}h_{t-1}+...だからdL/dz^h_t·h_{t-1}ᵀ。',
  },
  {
    id: 30,
    category: 'RNN・LSTM・GRU',
    question: 'LSTMの(あ)ゲートと(い)活性化関数の組み合わせとして最も適切な選択肢を選べ。',
    choices: [
      '(あ)忘却 (い)tanh関数',
      '(あ)リセット (い)tanh関数',
      '(あ)忘却 (い)シグモイド関数',
      '(あ)リセット (い)シグモイド関数',
    ],
    answer: 'C',
    hint: 'c_{t-1}の保持を制御→忘却ゲート（リセットはGRU）。ゲート=0〜1→シグモイド。',
  },
  {
    id: 31,
    category: 'RNN・LSTM・GRU',
    question: 'LSTMの c_t=(あ)、y_t=h_t=(い) の組み合わせとして最も適切な選択肢を選べ。',
    choices: [
      '(あ)f_t⊙ĉ_t+i_t⊙c_{t-1}  (い)o_t⊙σ(c_t)',
      '(あ)f_t⊙c_{t-1}+i_t⊙ĉ_t  (い)o_t⊙tanh(c_t)',
      '(あ)f_t⊙c_{t-1}+i_t⊙ĉ_t  (い)o_t⊙σ(c_t)',
      '(あ)f_t⊙ĉ_t+i_t⊙c_{t-1}  (い)o_t⊙tanh(c_t)',
    ],
    answer: 'B',
    hint: 'c_t=忘却×前セル+入力×候補。h_t=出力ゲート×tanh(c_t)。',
  },
  {
    id: 32,
    category: 'RNN・LSTM・GRU',
    question: 'GRUのz_tの役割(あ)と使用する活性化関数(い)の組み合わせとして最も適切な選択肢を選べ。',
    choices: [
      '(あ)リセットゲート、前の情報の保持を制御 (い)tanh関数',
      '(あ)リセットゲート、前の情報の保持を制御 (い)シグモイド関数',
      '(あ)更新ゲート、情報の更新量を制御 (い)tanh関数',
      '(あ)更新ゲート、情報の更新量を制御 (い)シグモイド関数',
    ],
    answer: 'C',
    hint: 'z_t=更新ゲート（前状態と新状態の混合比率）。ゲート=シグモイド。※問題の選択肢確認。',
  },
  {
    id: 33,
    category: 'RNN・LSTM・GRU',
    question: 'GRUの ĥ_t=(あ)、h_t=(い) の組み合わせとして最も適切な選択肢を選べ。',
    choices: [
      '(あ)tanh(W_ĥ r_t⊙h_{t-1}+W_ĥ x_t+b_ĥ) (い)(1−h_{t-1})⊙z_t+ĥ_t⊙z_t',
      '(あ)σ(W_ĥ r_t⊙h_{t-1}+W_ĥ x_t+b_ĥ) (い)(1−h_{t-1})⊙z_t+ĥ_t⊙z_t',
      '(あ)tanh(W_ĥ r_t⊙h_{t-1}+W_ĥ x_t+b_ĥ) (い)(1−z_t)⊙h_{t-1}+z_t⊙ĥ_t',
      '(あ)σ(W_ĥ r_t⊙h_{t-1}+W_ĥ x_t+b_ĥ) (い)(1−z_t)⊙h_{t-1}+z_t⊙ĥ_t',
    ],
    answer: 'C',
    hint: 'ĥ_t=候補状態でtanh。h_t=(1−z_t)⊙h_{t-1}+z_t⊙ĥ_t（更新ゲートで混合）。',
  },
  {
    id: 34,
    category: 'Transformer',
    question: 'Scaled Dot-Product Attentionの式として最も適切な選択肢を選べ。',
    choices: [
      'softmax(KQᵀ)V',
      'softmax(QVᵀ/√d_k + 1)K',
      'softmax(QVᵀ)K',
      'softmax(QKᵀ/√d_k)V',
    ],
    answer: 'D',
    hint: 'QKᵀ（QとKの内積）+ スケーリング√d_k + 最後にVを掛ける。',
  },
  {
    id: 35,
    category: 'Transformer',
    question: 'Transformerのエンコーダとデコーダの構造図として最も適切な選択肢を選べ。',
    choices: [
      'エンコーダにFeed Forwardがない構造',
      'エンコーダ: Multi-Head Attention→Feed Forward、デコーダ: Masked MHA→MHA→Feed Forward',
      'エンコーダのFeed Forwardが一番下にある構造',
      'デコーダのMulti-Head Attentionが一番下にある構造',
    ],
    answer: 'B',
    hint: 'エンコーダ: MHA→FF。デコーダ: Masked MHA→Cross MHA→FF。',
  },
  {
    id: 36,
    category: '正則化・初期化',
    question: 'バッチ正規化の分散σ²=(あ)と正規化x̂_i=(い)の組み合わせとして最も適切な選択肢を選べ。',
    choices: [
      '(あ)1/m Σ(x_i−μ)²  (い)(x_i+μ)/√(σ+ε)',
      '(あ)1/m Σ(x_i−μ)²  (い)(x_i−μ)/√(σ²+ε)',
      '(あ)1/m Σ(x_i²−μ²)  (い)(x_i+μ)/√(σ+ε)',
      '(あ)1/m Σ(x_i²−μ²)  (い)(x_i−μ)/√(σ²+ε)',
    ],
    answer: 'B',
    hint: '分散=(x_i−μ)²の平均。正規化=(x_i−μ)/√(σ²+ε)。引き算+σ²にルート。',
  },
  {
    id: 37,
    category: 'CNN',
    question: '残差ブロック（ResNet）の図として適切なものを選べ。',
    choices: [
      '2つの別関数F(x)+G(x)の出力',
      '入力xをスキップして出力F(x)+xに加算する構造',
      'スキップ接続なしの3層構造',
      '加算ではなく並列出力する構造',
    ],
    answer: 'B',
    hint: '残差接続=入力xをスキップして出力に加算→F(x)+x。',
  },
  {
    id: 38,
    category: '物体検出',
    question: 'Fast R-CNNのMulti-task lossの式として最も適切な選択肢を選べ。',
    choices: [
      'L = L_cls + λ[u≥0]L_loc',
      'L = L_cls + λ[u≥1]L_loc',
      'L = λ[u≥0]L_cls + L_loc',
      'L = λ[u≥1]L_cls + L_loc',
    ],
    answer: 'B',
    hint: 'u≥1で物体あり（u=0は背景）。L_clsはそのまま、L_locはλ付きで物体ある時だけ。',
  },
  {
    id: 39,
    category: '物体検出',
    question: 'YOLOの損失関数で、矩形の大きさの誤差(あ)と物体不在の信頼度(い)の組み合わせとして最も適切な選択肢を選べ。',
    choices: [
      '(あ)λ_coord Σ I^obj[(√w_i−√ŵ_i)²+(√h_i−√ĥ_i)²]  (い)Σ I^obj[(C_i−Ĉ_i)²]+λ_noobj Σ I^noobj[(C_i−Ĉ_i)²]',
      '(あ)λ_coord Σ I^obj[(x_i−x̂_i)²−(y_i−ŷ_i)²]  (い)λ_noobj Σ I^noobj[(C_i−Ĉ_i)²]',
      '(あ)λ_coord Σ I^obj[(√w_i−√ŵ_i)²+(√h_i−√ĥ_i)²]  (い)λ_noobj Σ I^noobj[(C_i−Ĉ_i)²]',
      '(あ)λ_coord Σ I^obj[(x_i−x̂_i)²−(y_i−ŷ_i)²]  (い)Σ I^obj[(C_i−Ĉ_i)²]+λ_noobj Σ I^noobj[(C_i−Ĉ_i)²]',
    ],
    answer: 'A',
    hint: '矩形サイズはルート付き(√w)で大小バランス。信頼度は物体あり+なし両方。',
  },
  {
    id: 40,
    category: '生成モデル',
    question: 'GANの学習を最適化問題として定式化した式として最も適切な選択肢を選べ。',
    choices: [
      'min_G max_D E_{x~p(x)}[logG(x)] + E_{z~p(z)}[log(1−D(z))]',
      'min_G max_D E_{x~p(x)}[logD(x)] + E_{z~p(z)}[log(1−D(G(z)))]',
      'min_D max_G E_{x~p(x)}[logG(x)] + E_{z~p(z)}[log(1−D(z))]',
      'min_D max_G E_{x~p(x)}[logD(x)] + E_{z~p(z)}[log(1−D(G(z)))]',
    ],
    answer: 'B',
    hint: 'Gはmin、Dはmax。第1項:本物にD→logD(x)。第2項:偽物に1−D(G(z))。',
  },
  {
    id: 41,
    category: '生成モデル',
    question: 'Wasserstein距離の定義式として最も適切な選択肢を選べ。',
    choices: [
      'W(p,q) = E_{(x,y)~γ}[sup_γ d(x,y)]',
      'W(p,q) = sup_γ E_{(x,y)~γ}[d(x,y)]',
      'W(p,q) = E_{(x,y)~γ}[inf_γ d(x,y)]',
      'W(p,q) = inf_γ E_{(x,y)~γ}[d(x,y)]',
    ],
    answer: 'D',
    hint: 'Wasserstein距離=最適輸送コストの下限→inf_γ。期待値は結合分布γの下で。',
  },
  {
    id: 42,
    category: '強化学習',
    question: 'モンテカルロ法のデメリット(あ)とTD学習の更新式(い)の組み合わせとして最も適切な選択肢を選べ。',
    choices: [
      '(あ)エピソード終了まで更新不可 (い)V\'(S_t)=V(S_t)+α{G_t−V(S_t)}',
      '(あ)全エピソードで計算膨大 (い)V\'(S_t)=V(S_t)+α{G_t−V(S_t)}',
      '(あ)エピソード終了まで更新不可 (い)V\'(S_t)=V(S_t)+α{R_t+γV(S_{t+1})−V(S_t)}',
      '(あ)全エピソードで計算膨大 (い)V\'(S_t)=V(S_t)+α{R_t+γV(S_{t+1})−V(S_t)}',
    ],
    answer: 'C',
    hint: 'MC法=エピソード終了まで待つ必要あり。TD=ブートストラップでステップごと更新。',
  },
  {
    id: 43,
    category: '強化学習',
    question: 'Q学習の更新式として最も適切な選択肢を選べ。',
    choices: [
      'Q̂*(s_t,a_t) = Q̂(s_t,a_t)+α(R_t+γmax_{a\'}Q̂(s_{t+1},a\')−Q̂(s_t,a_t))',
      'Q̂*(s_t,a_t) = Q̂(s_t,a_t)+α(γmax_{a\'}Q̂(s_{t+1},a\')−Q̂(s_t,a_t))',
      'Q̂*(s_t,a_t) = Q̂(s_t,a_t)+α(R_t+γmax_{a\'}Q̂(s_{t+1},a\')−Q̂(s_t,a_t))',
      'Q̂*(s_t,a_t) = Q̂(s_t,a_t)+α(R_t+γmax_{a\'}Q̂(s_{t+1},a\')−Q̂(s_t,a_t))',
    ],
    answer: 'A',
    hint: 'Q学習=方策オフ。R_t+γmax_{a\'}Q̂(s_{t+1},a\')がTDターゲット。',
  },
  {
    id: 44,
    category: '強化学習',
    question: '方策勾配定理の式(あ)と問題点(い)の組み合わせとして最も適切な選択肢を選べ。',
    choices: [
      '(あ)Δ_θlogπ(a|s,θ)(Q(s,a)−b(s))  (い)計算コストが非常に高い',
      '(あ)Δ_θlogπ(a|s,θ)(Q(s,a)−b(s))  (い)解析的に求められない',
      '(あ)E_π[Δ_θlogπ(a|s,θ)(Q(s,a)−b(s))]  (い)計算コストが非常に高い',
      '(あ)E_π[Δ_θlogπ(a|s,θ)(Q(s,a)−b(s))]  (い)解析的に求められない',
    ],
    answer: 'D',
    hint: '方策勾配=期待値E_πの形。問題点=期待値なので解析的に求められない→サンプリング近似。',
  },
  {
    id: 45,
    category: '強化学習',
    question: 'Actor-Criticの(あ)具体的な手法と(い)Criticの定義の組み合わせとして最も適切な選択肢を選べ。',
    choices: [
      '(あ)方策と価値を並行して同時に学習 (い)価値ベースで学習したQ_{θ\'}(s,a)',
      '(あ)方策と価値を並行して同時に学習 (い)報酬の予測モデルΔ_θlogπ(a_t|s_t)',
      '(あ)方策と価値を交互に学習 (い)価値ベースで学習したQ_{θ\'}(s,a)',
      '(あ)方策と価値を交互に学習 (い)報酬の予測モデルΔ_θlogπ(a_t|s_t)',
    ],
    answer: 'A',
    hint: 'Actor-Critic=方策と価値を並行（同時）に学習。Critic=価値ベースのQ関数。',
  },
  {
    id: 46,
    category: '説明可能AI',
    question: 'LIMEの説明モデルg(z)=(あ)と画像での適用(い)の組み合わせとして最も適切な選択肢を選べ。',
    choices: [
      '(あ)Σφ_i  (い)似た色・テクスチャのピクセル集合',
      '(あ)Σφ_i  (い)ランダムにノイズ置換した入力全体',
      '(あ)φ₀+Σφ_iz_i  (い)似た色・テクスチャのピクセル集合',
      '(あ)φ₀+Σφ_iz_i  (い)ランダムにノイズ置換した入力全体',
    ],
    answer: 'C',
    hint: 'g(z)=φ₀+Σφ_iz_i（切片あり線形モデル）。画像=スーパーピクセル（似た領域の集合）。',
  },
  {
    id: 47,
    category: '説明可能AI',
    question: 'LIMEの目的関数(あ)と第2項の役割(い)の組み合わせとして最も適切な選択肢を選べ。',
    choices: [
      '(あ)min_φ Σ|g(z)−f(x\')|²+λΣ|φ_i|  (い)いくつかの寄与度を0にする',
      '(あ)min_φ Σ|g(z)−f(x\')|²+λΣ|φ_i|  (い)各寄与度を均等にする',
      '(あ)min_{x\'} Σ|g(z)−f(x)|²+λΣ|x\'|  (い)いくつかの寄与度を0にする',
      '(あ)min_{x\'} Σ|g(z)−f(x)|²+λΣ|x\'|  (い)各寄与度を均等にする',
    ],
    answer: 'A',
    hint: '最小化対象はφ。L1正則化λΣ|φ_i|→スパース化=一部の寄与度を0にする。',
  },
  {
    id: 48,
    category: '物体検出',
    question: 'SSDの損失関数(あ)と背景の分類誤差(い)の組み合わせとして最も適切な選択肢を選べ。',
    choices: [
      '(あ)L=(1/N)L_conf+αL_loc  (い)log(Ĉ_i⁰)',
      '(あ)L=(1/N)L_conf+αL_loc  (い)x^p_{ij}log(Ĉ^p_i)',
      '(あ)L=αL_conf+(1/N)L_loc  (い)x^p_{ij}log(Ĉ^p_i)',
      '(あ)L=αL_conf+(1/N)L_loc  (い)log(Ĉ_i⁰)',
    ],
    answer: 'A',
    hint: 'SSD: confが1/N、locがα。背景誤差=log(Ĉ_i⁰)（クラス0=背景、x^p_ijなし）。',
  },
  {
    id: 49,
    category: '物体検出',
    question: 'FCOSのセンターネス(あ)とクラス分類の損失関数(い)の組み合わせとして最も適切な選択肢を選べ。',
    choices: [
      '(あ)√(min(l*,r*)/max(l*,r*)×min(t*,b*)/max(t*,b*))  (い)focal loss',
      '(あ)√(max(l*,r*)/min(l*,r*)×max(t*,b*)/min(t*,b*))  (い)IoU loss',
      '(あ)√(max(l*,r*)/min(l*,r*)×max(t*,b*)/min(t*,b*))  (い)focal loss',
      '(あ)√(min(l*,r*)/max(l*,r*)×min(t*,b*)/max(t*,b*))  (い)IoU loss',
    ],
    answer: 'A',
    hint: 'センターネス=min/max（中心に近いほど1）。L_cls=focal loss（クラス不均衡対策）。',
  },
  {
    id: 50,
    category: '生成モデル',
    question: 'GANの問題点について、以下のうち最も不適切な選択肢を選べ。',
    choices: [
      'モード崩壊とは、生成器が特定のデータしか生成しなくなる問題。',
      '正規化なしで学習すると画像内の特定の傾向を学習してしまう場合がある。',
      '生成器と識別器が適切な均衡点に収束しない場合がある。',
      '識別器が生成器より優れている場合、勾配が消失し学習できなくなる。',
    ],
    answer: 'B',
    hint: 'BはGAN特有の問題ではなく一般的なNN学習の話。A,C,DはGAN固有の課題。',
  },
];
